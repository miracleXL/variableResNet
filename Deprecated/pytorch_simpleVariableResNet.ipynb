{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchinfo\n",
    "from multiprocessing import Process\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device:【cuda:None】\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU Device:【{}:{}】\".format(device.type, device.index))\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\"MNIST_data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(\"MNIST_data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([32, 1, 28, 28])\n",
      "Shape of y: torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, device=None, *args, **wargs):\n",
    "        super().__init__(*args, **wargs)\n",
    "        self.__built = False\n",
    "        self.device = device\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        # resolve output shape in model summary\n",
    "        self.conv = nn.Conv2d(input_shape[1], 1, 5, padding=\"same\")\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(input_shape[1])\n",
    "        self.downconv = nn.Conv2d(input_shape[1], 1, 5, padding=\"same\")\n",
    "        self.downbn = nn.BatchNorm2d(input_shape[1])\n",
    "        self.output_shape = input_shape\n",
    "        self.to(self.device)\n",
    "        self.__built = True\n",
    "\n",
    "    def forward(self, inputs:np.ndarray):\n",
    "        x:np.ndarray = inputs\n",
    "        fx:np.ndarray = x\n",
    "        fx = self.conv(fx)\n",
    "        fx = self.bn(fx)\n",
    "        if fx.shape[-1] != x.shape[-1]:\n",
    "            x = self.downconv(x)\n",
    "            x = self.downbn(x)\n",
    "        try:\n",
    "            # print(self.name, x.shape, fx.shape, inputs.shape)\n",
    "            return fx + x\n",
    "        except:\n",
    "            raise RuntimeError(x.shape, fx.shape, inputs.shape)\n",
    "\n",
    "    # def get_weights(self):\n",
    "    #     return [self.conv.get_weights(), self.bn.get_weights()]\n",
    "\n",
    "    # def set_weights(self, weights:list):\n",
    "    #     self.conv.set_weights(weights[0])\n",
    "    #     self.bn.set_weights(weights[1])\n",
    "    #     return super().set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, device=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__blocks_num = 1\n",
    "        self.__frozen_blocks_num = 0\n",
    "        self.__built = False\n",
    "        self.device = device\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks = [ResBlock()]\n",
    "        for block in self.blocks:\n",
    "            block.build(input_shape)\n",
    "            block.to(self.device)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(np.prod(input_shape[1:]), 10)\n",
    "        self.__built = True\n",
    "\n",
    "    def compile(self, dataloader:DataLoader, loss_fn, optimizer):\n",
    "        self.batch_size:int = dataloader.batch_size\n",
    "        for X, y in dataloader:\n",
    "            self.input_shape:tuple = X.shape\n",
    "            self.output_shape:tuple = y.shape\n",
    "            break\n",
    "        self.build(self.input_shape)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(self.parameters(), lr=1e-2)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # for i in range(self.__blocks_num):\n",
    "        #     x = self.blocks[i](x)\n",
    "        # x = self.blocks[0](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def getBlocksNum(self):\n",
    "        return self.__blocks_num\n",
    "\n",
    "    def addNewBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"Adding new block...\")\n",
    "        if self.__blocks_num >= len(self.blocks):\n",
    "            newBlock = ResBlock()\n",
    "            newBlock.build(self.blocks[-1].output_shape)\n",
    "            self.blocks.append(newBlock)\n",
    "            newBlock.to(self.device)\n",
    "        self.__blocks_num += 1\n",
    "        print(\"Success!\")\n",
    "\n",
    "    def copyLastBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"Copying last block...\")\n",
    "        newBlock = ResBlock()\n",
    "        last_block:ResBlock = self.blocks[-1]\n",
    "        newBlock.build(last_block.output_shape)\n",
    "        if last_block.input_shape == last_block.output_shape:\n",
    "            newBlock.load_state_dict(last_block.state_dict())\n",
    "        else:\n",
    "            print(\"Copy failed: shape different with last block\")\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num += 1\n",
    "        print(\"Success!\")\n",
    "\n",
    "    def fit(self, dataloader:DataLoader, epochs:int=1):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = size // self.batch_size\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch}/{epochs}\")\n",
    "            loss, correct = 0, 0\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = self.forward(X)\n",
    "                batch_loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                current = batch * self.batch_size + len(X)\n",
    "\n",
    "                batch_loss = batch_loss.item()\n",
    "                loss += batch_loss\n",
    "                batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                correct += batch_correct\n",
    "\n",
    "                batch_correct /= self.batch_size\n",
    "                print(f\"\\r{batch+1}/{num_batches+1}  [{current:>5d}/{size:>5d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}%\", end = \"\")\n",
    "            loss /= num_batches\n",
    "            correct /= size\n",
    "            print(f\"\\nAverage loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}%\")\n",
    "        torchinfo.summary(self, input_size=self.input_shape)\n",
    "\n",
    "    def test(self, dataloader:DataLoader):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        self.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyResNet(device)\n",
    "model.compile(train_dataloader, nn.CrossEntropyLoss, torch.optim.SGD)\n",
    "model.fit(train_dataloader, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
