{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model, layers\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + current_time\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "# test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "# train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "# test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images dataset\n",
    "def load_dataset(name:str=\"mnist\", size:int=None):\n",
    "    if name == \"mnist\":\n",
    "        (train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "    elif name == \"cifar10\":\n",
    "        (train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "    train_x, test_x = train_x/255.0, test_x/255.0\n",
    "\n",
    "    if size:\n",
    "        train_x = train_x[:size][..., tf.newaxis].astype(\"float32\")\n",
    "        test_x = test_x[:size][..., tf.newaxis].astype(\"float32\")\n",
    "        train_y, test_y = train_y[:size], test_y[:size]\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=30\n",
    "(train_x, train_y), (test_x, test_y) = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, filters, kernel_size, strides = (1, 1), padding: str = 'same', *args, **wargs):\n",
    "        super().__init__(*args, **wargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        # convolution\n",
    "        self.conv1 = layers.Conv2D(self.filters, self.kernel_size, strides=self.strides, padding=self.padding, activation=\"relu\")\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(self.filters, self.kernel_size, padding=self.padding)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        # self.conv3 = layers.Conv2D(self.filters, self.kernel_size, padding=self.padding, activation=\"relu\")\n",
    "        # self.bn3 = layers.BatchNormalization()\n",
    "        # shortcut\n",
    "        self.downconv = layers.Conv2D(self.filters, 1, padding=self.padding)\n",
    "        self.downbn = layers.BatchNormalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # resolve output shape in model summary\n",
    "        input_layer = layers.Input(shape=input_shape[1:], batch_size=input_shape[0])\n",
    "        self.call(input_layer)\n",
    "        return super().build(input_shape)\n",
    "    \n",
    "    def shortcut(self, x):\n",
    "        x = self.downconv(x)\n",
    "        x = self.downbn(x)\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs:np.ndarray, training=None, mask=None):\n",
    "        x:np.ndarray = inputs\n",
    "        fx:np.ndarray = x\n",
    "        # f(x)\n",
    "        fx = self.conv1(fx, training=training)\n",
    "        fx = self.bn1(fx, training=training)\n",
    "        fx = self.conv2(fx, training=training)\n",
    "        fx = self.bn2(fx, training=training)\n",
    "        # fx = self.conv3(fx, training=training)\n",
    "        # fx = self.bn3(fx, training=training)\n",
    "        # h(x) = x + f(x)\n",
    "        if fx.shape[-1] != x.shape[-1]:\n",
    "            # x = self.downconv(x, training=training)\n",
    "            # x = self.downbn(x, training=training)\n",
    "            x = self.shortcut(x)\n",
    "        try:\n",
    "            return fx + x\n",
    "        except:\n",
    "            raise RuntimeError(x.shape, fx.shape, inputs.shape)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return [self.conv1.get_weights(), self.bn1.get_weights(),\n",
    "                self.conv2.get_weights(), self.bn2.get_weights()\n",
    "        ]\n",
    "    \n",
    "    def set_weights(self, weights:list):\n",
    "        self.conv1.set_weights(weights[0])\n",
    "        self.bn1.set_weights(weights[1])\n",
    "        self.conv2.set_weights(weights[2])\n",
    "        self.bn2.set_weights(weights[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_34(Model):\n",
    "    def __init__(self, units, blocks=None, dynamic=True, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__blocks_num = 1\n",
    "        self.__frozen_blocks_num = 0\n",
    "        self.__units = units\n",
    "        self.dynamic_depts = dynamic\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.conv0 = layers.Conv2D(64, 7, strides=2, padding=\"same\", name=\"conv0\")\n",
    "        self.blocks1 = [\n",
    "            ResBlock(64, 3, name=\"res_block_64_0\"),\n",
    "            ResBlock(64, 3, name=\"res_block_64_1\"),\n",
    "            ResBlock(64, 3, name=\"res_block_64_2\")\n",
    "        ]\n",
    "        self.blocks2 = [\n",
    "            ResBlock(128, 3, name=\"res_block_128_3\"),\n",
    "            ResBlock(128, 3, name=\"res_block_128_4\"),\n",
    "            ResBlock(128, 3, name=\"res_block_128_5\"),\n",
    "            ResBlock(128, 3, name=\"res_block_128_6\")\n",
    "        ]\n",
    "        self.blocks3 = [\n",
    "            ResBlock(256, 3, name=\"res_block_256_7\"),\n",
    "            ResBlock(256, 3, name=\"res_block_256_8\"),\n",
    "            ResBlock(256, 3, name=\"res_block_256_9\"),\n",
    "            ResBlock(256, 3, name=\"res_block_256_10\"),\n",
    "            ResBlock(256, 3, name=\"res_block_256_11\"),\n",
    "            ResBlock(256, 3, name=\"res_block_256_12\")\n",
    "        ]\n",
    "        self.blocks4 = [\n",
    "            ResBlock(512, 3, name=\"res_block_512_13\"),\n",
    "            ResBlock(512, 3, name=\"res_block_512_14\"),\n",
    "            ResBlock(512, 3, name=\"res_block_512_15\"),\n",
    "        ]\n",
    "        self.blocks = blocks\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(units)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # resolve output shape in model summary\n",
    "        input_layer = layers.Input(shape=input_shape[1:], batch_size=input_shape[0])\n",
    "        self.call(input_layer)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.conv0(x, training=training)\n",
    "        if self.dynamic_depts:\n",
    "            if self.blocks is None:\n",
    "                # print(\"called dynamic\")\n",
    "                for i in range(min(len(self.blocks1), self.__blocks_num)):\n",
    "                    x = self.blocks1[i](x, training=training)\n",
    "                for i in range(min(len(self.blocks2), self.__blocks_num)):\n",
    "                    x = self.blocks2[i](x, training=training)\n",
    "                for i in range(min(len(self.blocks3), self.__blocks_num)):\n",
    "                    x = self.blocks3[i](x, training=training)\n",
    "                for i in range(min(len(self.blocks4), self.__blocks_num)):\n",
    "                    x = self.blocks4[i](x, training=training)\n",
    "            else:\n",
    "                for i in range(self.__blocks_num):\n",
    "                    x = self.blocks[i](x, training=training)\n",
    "        else:\n",
    "            if self.blocks is None:\n",
    "                # print(\"called static\")\n",
    "                for block in self.blocks1:\n",
    "                    x = block(x, training=training)\n",
    "                for block in self.blocks2:\n",
    "                    x = block(x, training=training)\n",
    "                for block in self.blocks3:\n",
    "                    x = block(x, training=training)\n",
    "                for block in self.blocks4:\n",
    "                    x = block(x, training=training)\n",
    "            else:\n",
    "                for block in self.blocks:\n",
    "                    x = block(x, training=training)\n",
    "        x = self.flatten(x, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def getBlocksNum(self):\n",
    "        return self.__blocks_num\n",
    "    \n",
    "    def freezeBlocks(self, num):\n",
    "        for i in range(self.__frozen_blocks_num, min(self.__frozen_blocks_num+num, self.__blocks_num)):\n",
    "            if self.blocks is None:\n",
    "                if len(self.blocks1) > self.__blocks_num:\n",
    "                    self.blocks1[i].trainable = False\n",
    "                if len(self.blocks2) > self.__blocks_num:\n",
    "                    self.blocks2[i].trainable = False\n",
    "                if len(self.blocks3) > self.__blocks_num:\n",
    "                    self.blocks3[i].trainable = False\n",
    "                if len(self.blocks4) > self.__blocks_num:\n",
    "                    self.blocks4[i].trainable = False\n",
    "            else:\n",
    "                self.blocks[i].trainable = False\n",
    "        self.__frozen_blocks_num = min(self.__frozen_blocks_num+num, self.__blocks_num)\n",
    "\n",
    "    def addNewBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"add new blocks\")\n",
    "        self.freezeBlocks(1)\n",
    "        self.__blocks_num += 1\n",
    "        print(f\"this is the {self.__blocks_num} added blocks\")#, block name: {self.blocks[self.__blocks_num-1].name}\")\n",
    "    \n",
    "    def copyLastBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"copy last block\")\n",
    "        self.freezeBlocks(1)\n",
    "        if self.blocks is not None:\n",
    "            return\n",
    "        if len(self.blocks1) > self.__blocks_num:\n",
    "            newBlock = self.blocks1[self.__blocks_num]\n",
    "            last_block:ResBlock = self.blocks1[self.__blocks_num-1]\n",
    "            newBlock(last_block.output)\n",
    "            if last_block.input_shape == newBlock.input_shape and last_block.output_shape == newBlock.output_shape:\n",
    "                newBlock.set_weights(last_block.get_weights())\n",
    "            else:\n",
    "                print(\"block1 copy failed: shape different with last block\")\n",
    "        if len(self.blocks2) > self.__blocks_num:\n",
    "            newBlock = self.blocks2[self.__blocks_num]\n",
    "            last_block:ResBlock = self.blocks2[self.__blocks_num-1]\n",
    "            newBlock(last_block.output)\n",
    "            if last_block.input_shape == newBlock.input_shape and last_block.output_shape == newBlock.output_shape:\n",
    "                newBlock.set_weights(last_block.get_weights())\n",
    "            else:\n",
    "                print(\"block2 copy failed: shape different with last block\")\n",
    "        if len(self.blocks3) > self.__blocks_num:\n",
    "            newBlock = self.blocks3[self.__blocks_num]\n",
    "            last_block:ResBlock = self.blocks3[self.__blocks_num-1]\n",
    "            newBlock(last_block.output)\n",
    "            if last_block.input_shape == newBlock.input_shape and last_block.output_shape == newBlock.output_shape:\n",
    "                newBlock.set_weights(last_block.get_weights())\n",
    "            else:\n",
    "                print(\"block3 copy failed: shape different with last block\")\n",
    "        if len(self.blocks4) > self.__blocks_num:\n",
    "            newBlock = self.blocks4[self.__blocks_num]\n",
    "            last_block:ResBlock = self.blocks4[self.__blocks_num-1]\n",
    "            newBlock(last_block.output)\n",
    "            if last_block.input_shape == newBlock.input_shape and last_block.output_shape == newBlock.output_shape:\n",
    "                newBlock.set_weights(last_block.get_weights())\n",
    "            else:\n",
    "                print(\"block4 copy failed: shape different with last block\")\n",
    "        self.__blocks_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamicResNet:\n",
    "    def __init__(self, is_dynamic=True, condition: types.FunctionType = None, max_blocks_num:int = 2, copy_last_block:bool = False,*args, **wargs) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            is_dynamic: bool, this model's depth should be dynamical increase or not\n",
    "                模型深度是否动态增加\n",
    "            condition: A function, which will be called in every epoch and returns a boolean value representing whether to add a new block.\n",
    "                每个epoch会被调用一次，返回值为布尔类型，代表是否添加新的块\n",
    "            max_blocks_num: int, total num of blocks which will be added into model in the last\n",
    "                最终会被添加到模型中的总残差块数\n",
    "            copy_last_block: bool, whether copy the last block's weight to new block\n",
    "                新的block是否复制最后一个block的权重\n",
    "        \"\"\"\n",
    "        super(dynamicResNet, self).__init__(*args, **wargs)\n",
    "        self.dynamic = is_dynamic\n",
    "        if condition is None:\n",
    "            self.add_condition = self.set_epochs\n",
    "            self.add_condition()\n",
    "        else:\n",
    "            if callable(condition):\n",
    "                self.add_condition = condition\n",
    "            else:\n",
    "                raise ValueError(\"'condition' must be a function\")\n",
    "        self.max_blocks_num = max_blocks_num\n",
    "        self.copy_last_block = copy_last_block\n",
    "        # build model //创建模型\n",
    "        self.model = ResNet_34(10, dynamic=self.dynamic)\n",
    "        self.compiled = False\n",
    "\n",
    "    def compile(self,\n",
    "                optimizer=\"rmsprop\",\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                loss_weights=None,\n",
    "                weighted_metrics=None,\n",
    "                run_eagerly=None,\n",
    "                steps_per_execution=None,\n",
    "                **kwargs\n",
    "    ):\n",
    "        self.complieArgs = [optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution]\n",
    "        self.complieKwargs = kwargs\n",
    "        self.model.compile(*self.complieArgs, **kwargs)\n",
    "        self.compiled = True\n",
    "\n",
    "    def fit(self,\n",
    "            x=None,\n",
    "            y=None,\n",
    "            batch_size=None,\n",
    "            epochs=1,\n",
    "            verbose=\"auto\",\n",
    "            callbacks=None,\n",
    "            validation_split=0.0,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_batch_size=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False\n",
    "    ):\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError(\"model should be compiled before fit\")\n",
    "        self.epochs = epochs\n",
    "        self.fitArgs = [x,y,batch_size,1,verbose,callbacks,validation_split,validation_data,shuffle,class_weight,sample_weight,initial_epoch,steps_per_epoch,validation_steps,validation_batch_size,validation_freq,max_queue_size,workers,use_multiprocessing]\n",
    "        return self.call(training=True)\n",
    "    \n",
    "    def predict(self,\n",
    "                x,\n",
    "                batch_size=None,\n",
    "                verbose=\"auto\",\n",
    "                steps=None,\n",
    "                callbacks=None,\n",
    "                max_queue_size=10,\n",
    "                workers=1,\n",
    "                use_multiprocessing=False\n",
    "    ):\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError(\"model should be compiled before predict\")\n",
    "        return self.model.predict( x,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=verbose,\n",
    "                                    steps=steps,\n",
    "                                    callbacks=callbacks,\n",
    "                                    max_queue_size=max_queue_size,\n",
    "                                    workers=workers,\n",
    "                                    use_multiprocessing=use_multiprocessing\n",
    "                                 )\n",
    "\n",
    "    def call(self, x=None, training=False):\n",
    "        if training:\n",
    "            def fit_epoch():\n",
    "                # 满足条件动态添加新残差块\n",
    "                if self.model.getBlocksNum() < self.max_blocks_num and self.add_condition():\n",
    "                    if self.copy_last_block:\n",
    "                        self.model.copyLastBlock()\n",
    "                    else:\n",
    "                        self.model.addNewBlock()\n",
    "                    self.model.compile(*self.complieArgs, **self.complieKwargs)\n",
    "                self.model.fit(*self.fitArgs)\n",
    "            for epoch in range(self.epochs):\n",
    "                print(\"Epoch: \", epoch)\n",
    "                p = Thread(target=fit_epoch)\n",
    "                p.start()\n",
    "                p.join()\n",
    "        else:\n",
    "            return self.model.predict(x)\n",
    "\n",
    "    def set_epochs(self, interval_of_epochs:int = None) -> None:\n",
    "        self.epoch = 0\n",
    "        self.last_change_epoch = 1\n",
    "        if interval_of_epochs is None:\n",
    "            self.interval = 1\n",
    "        else:\n",
    "            self.interval = interval_of_epochs\n",
    "        self.add_condition = self.__num_of_epochs\n",
    "\n",
    "    def __num_of_epochs(self) -> bool:\n",
    "        self.epoch += 1\n",
    "        if self.epoch - self.last_change_epoch == self.interval:\n",
    "            self.last_change_epoch = self.epoch\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ResNet_34.call of <__main__.ResNet_34 object at 0x00000209C2F9F0F0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ResNet_34.call of <__main__.ResNet_34 object at 0x00000209C2F9F0F0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1563/1563 [==============================] - 56s 21ms/step - loss: 4.7525 - accuracy: 0.3128\n",
      "Epoch:  1\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.4312 - accuracy: 0.5166\n",
      "Epoch:  2\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.3369 - accuracy: 0.5698\n",
      "Epoch:  3\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.2863 - accuracy: 0.6032\n",
      "Epoch:  4\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.1684 - accuracy: 0.6358\n",
      "Epoch:  5\n",
      "----------\n",
      "copy last block\n",
      "1 (None, 16, 16, 64) (None, 16, 16, 64) (None, 16, 16, 64) (None, 16, 16, 64) (None, 16, 16, 64)\n",
      "block2 copy failed: shape different with last block\n",
      "block3 copy failed: shape different with last block\n",
      "block4 copy failed: shape different with last block\n",
      "1563/1563 [==============================] - 55s 33ms/step - loss: 14.4548 - accuracy: 0.4632\n",
      "Epoch:  6\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 2.3082 - accuracy: 0.5870\n",
      "Epoch:  7\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 1.0942 - accuracy: 0.6655\n",
      "Epoch:  8\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 0.9510 - accuracy: 0.6898\n",
      "Epoch:  9\n",
      "1563/1563 [==============================] - 51s 33ms/step - loss: 1.1844 - accuracy: 0.6657\n",
      "Epoch:  10\n",
      "----------\n",
      "copy last block\n",
      "2 (None, 16, 16, 64) (None, 16, 16, 64) (None, 16, 16, 64) (None, 16, 16, 64) (None, 16, 16, 64)\n",
      "1563/1563 [==============================] - 73s 44ms/step - loss: 2.6962 - accuracy: 0.6499\n",
      "Epoch:  11\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.8882 - accuracy: 0.7107\n",
      "Epoch:  12\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7897 - accuracy: 0.7305\n",
      "Epoch:  13\n",
      "1563/1563 [==============================] - 68s 44ms/step - loss: 0.7972 - accuracy: 0.7323\n",
      "Epoch:  14\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 0.7190 - accuracy: 0.7507\n",
      "Epoch:  15\n",
      "----------\n",
      "copy last block\n",
      "1563/1563 [==============================] - 79s 47ms/step - loss: 0.7958 - accuracy: 0.7551\n",
      "Epoch:  16\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 5.8394 - accuracy: 0.6684\n",
      "Epoch:  17\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 1.6793 - accuracy: 0.5058\n",
      "Epoch:  18\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 1.3018 - accuracy: 0.5700\n",
      "Epoch:  19\n",
      "1563/1563 [==============================] - 75s 48ms/step - loss: 1.1567 - accuracy: 0.6053\n",
      "Epoch:  20\n",
      "----------\n",
      "copy last block\n",
      "1563/1563 [==============================] - 85s 51ms/step - loss: 1.1081 - accuracy: 0.6303\n",
      "Epoch:  21\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.9989 - accuracy: 0.7180\n",
      "Epoch:  22\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6664 - accuracy: 0.7678\n",
      "Epoch:  23\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.5915 - accuracy: 0.7926\n",
      "Epoch:  24\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.5392 - accuracy: 0.8114\n",
      "Epoch:  25\n",
      "----------\n",
      "copy last block\n",
      "1563/1563 [==============================] - 90s 54ms/step - loss: 0.4892 - accuracy: 0.8335\n",
      "Epoch:  26\n",
      "1563/1563 [==============================] - 85s 54ms/step - loss: 0.4577 - accuracy: 0.8410\n",
      "Epoch:  27\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 0.4920 - accuracy: 0.8332\n",
      "Epoch:  28\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.3737 - accuracy: 0.8683\n",
      "Epoch:  29\n",
      "1563/1563 [==============================] - 80s 51ms/step - loss: 0.3357 - accuracy: 0.8821\n",
      "Model: \"res_net_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv0 (Conv2D)               (None, 16, 16, 64)        9472      \n",
      "_________________________________________________________________\n",
      "res_block_64_0 (ResBlock)    (None, 16, 16, 64)        74368     \n",
      "_________________________________________________________________\n",
      "res_block_64_1 (ResBlock)    (None, 16, 16, 64)        74368     \n",
      "_________________________________________________________________\n",
      "res_block_64_2 (ResBlock)    (None, 16, 16, 64)        74368     \n",
      "_________________________________________________________________\n",
      "res_block_128_3 (ResBlock)   (None, 16, 16, 128)       231296    \n",
      "_________________________________________________________________\n",
      "res_block_128_4 (ResBlock)   (None, 16, 16, 128)       296192    \n",
      "_________________________________________________________________\n",
      "res_block_128_5 (ResBlock)   (None, 16, 16, 128)       296192    \n",
      "_________________________________________________________________\n",
      "res_block_128_6 (ResBlock)   (None, 16, 16, 128)       296192    \n",
      "_________________________________________________________________\n",
      "res_block_256_7 (ResBlock)   (None, 16, 16, 256)       921344    \n",
      "_________________________________________________________________\n",
      "res_block_256_8 (ResBlock)   (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_9 (ResBlock)   (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_10 (ResBlock)  (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_11 (ResBlock)  (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_12 (ResBlock)  (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_512_13 (ResBlock)  (None, 16, 16, 512)       3677696   \n",
      "_________________________________________________________________\n",
      "res_block_512_14 (ResBlock)  (None, 16, 16, 512)       4723712   \n",
      "_________________________________________________________________\n",
      "res_block_512_15 (ResBlock)  (None, 16, 16, 512)       4723712   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1310730   \n",
      "=================================================================\n",
      "Total params: 22,620,682\n",
      "Trainable params: 7,592,842\n",
      "Non-trainable params: 15,027,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dynamic_model = dynamicResNet(is_dynamic=True, max_blocks_num=6, copy_last_block=True)\n",
    "dynamic_model.set_epochs(5)\n",
    "def fit_dynamic_model():\n",
    "    dynamic_model.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    dynamic_model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "p = Thread(target=fit_dynamic_model)\n",
    "p.start()\n",
    "p.join()\n",
    "dynamic_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 113s 65ms/step - loss: 7.4278 - accuracy: 0.2330\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 1.7156 - accuracy: 0.4234\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 105s 67ms/step - loss: 1.7297 - accuracy: 0.4801\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 105s 67ms/step - loss: 1.6861 - accuracy: 0.5500\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.2841 - accuracy: 0.6073\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.4815 - accuracy: 0.6356s - loss: 1.4817 - accura\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.1494 - accuracy: 0.6776\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.0052 - accuracy: 0.7007\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.0142 - accuracy: 0.7183\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.0875 - accuracy: 0.7304\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.9415 - accuracy: 0.7416\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.8406 - accuracy: 0.7553\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.8235 - accuracy: 0.7702\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.6720 - accuracy: 0.8000\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.7161 - accuracy: 0.7994\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 0.7116 - accuracy: 0.7962\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.5408 - accuracy: 0.8412\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.5102 - accuracy: 0.8456\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4942 - accuracy: 0.8578\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.5958 - accuracy: 0.8480\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4443 - accuracy: 0.8772\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.3367 - accuracy: 0.9003\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.3338 - accuracy: 0.8990\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.4089 - accuracy: 0.8941\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2850 - accuracy: 0.9206\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2728 - accuracy: 0.9240\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2595 - accuracy: 0.9305\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2572 - accuracy: 0.9299\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.3028 - accuracy: 0.9260\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 0.2121 - accuracy: 0.9435\n",
      "Model: \"res_net_34_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv0 (Conv2D)               (None, 16, 16, 64)        9472      \n",
      "_________________________________________________________________\n",
      "res_block_64_0 (ResBlock)    (None, 16, 16, 64)        74368     \n",
      "_________________________________________________________________\n",
      "res_block_64_1 (ResBlock)    (None, 16, 16, 64)        74368     \n",
      "_________________________________________________________________\n",
      "res_block_64_2 (ResBlock)    (None, 16, 16, 64)        74368     \n",
      "_________________________________________________________________\n",
      "res_block_128_3 (ResBlock)   (None, 16, 16, 128)       231296    \n",
      "_________________________________________________________________\n",
      "res_block_128_4 (ResBlock)   (None, 16, 16, 128)       296192    \n",
      "_________________________________________________________________\n",
      "res_block_128_5 (ResBlock)   (None, 16, 16, 128)       296192    \n",
      "_________________________________________________________________\n",
      "res_block_128_6 (ResBlock)   (None, 16, 16, 128)       296192    \n",
      "_________________________________________________________________\n",
      "res_block_256_7 (ResBlock)   (None, 16, 16, 256)       921344    \n",
      "_________________________________________________________________\n",
      "res_block_256_8 (ResBlock)   (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_9 (ResBlock)   (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_10 (ResBlock)  (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_11 (ResBlock)  (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_256_12 (ResBlock)  (None, 16, 16, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "res_block_512_13 (ResBlock)  (None, 16, 16, 512)       3677696   \n",
      "_________________________________________________________________\n",
      "res_block_512_14 (ResBlock)  (None, 16, 16, 512)       4723712   \n",
      "_________________________________________________________________\n",
      "res_block_512_15 (ResBlock)  (None, 16, 16, 512)       4723712   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1310730   \n",
      "=================================================================\n",
      "Total params: 22,620,682\n",
      "Trainable params: 22,603,786\n",
      "Non-trainable params: 16,896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "static_model = ResNet_34(units=10, dynamic=False)\n",
    "def fit_static_model():\n",
    "    static_model.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    static_model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "p = Thread(target=fit_static_model)\n",
    "p.start()\n",
    "p.join()\n",
    "static_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "14da249aea668dd476485a6222a8c1fd2f2cd90c7005afa5442c69b6c853f2dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
