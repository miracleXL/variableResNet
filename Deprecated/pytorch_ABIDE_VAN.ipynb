{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchinfo\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device:【cuda:None】\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU Device:【{}:{}】\".format(device.type, device.index))\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data:np.ndarray, labels:np.ndarray, transform=ToTensor(), \n",
    "    target_transform=Lambda(lambda y: torch.zeros(2, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))):\n",
    "        self.data:torch.Tensor = torch.from_numpy(data)\n",
    "        self.labels:torch.Tensor = torch.from_numpy(labels)\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "        # self.transform = transform\n",
    "        # self.target_transform = target_transform\n",
    "        # self.shuffle()\n",
    "    \n",
    "    def shuffle(self, seed=None):\n",
    "        '\\n        seed(self, seed=None)\\n\\n        Reseed a legacy MT19937 BitGenerator\\n        '\n",
    "        self.shuffle_seed = np.random.randint(1, 65535) if seed is None else seed\n",
    "        print(f\"随机种子：{self.shuffle_seed}\")\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.data)\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx, 0]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=\"dataset.npz\", train_percent=0.8) -> tuple:\n",
    "    with np.load(path) as dataset:\n",
    "        full_data = dataset[\"data\"].astype(np.float32).reshape((-1, 1, 116, 116))\n",
    "        full_labels = dataset[\"labels\"].astype(np.int64)\n",
    "    train_size = int(full_data.shape[0]*train_percent)\n",
    "    test_size = full_data.shape[0]-train_size\n",
    "    seed = np.random.randint(1, 65535)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_data)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_labels)\n",
    "    train_data, test_data = full_data[:train_size], full_data[train_size:]\n",
    "    train_labels, test_labels = full_labels[:train_size], full_labels[train_size:]\n",
    "    print(f\"训练集大小：{train_size}\", f\"测试集大小：{test_size}\", f\"随机种子：{seed}\")\n",
    "    train_dataset = CustomDataset(train_data, train_labels)\n",
    "    test_dataset = CustomDataset(test_data, test_labels)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：9636 测试集大小：2409 随机种子：26930\n"
     ]
    }
   ],
   "source": [
    "# train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_FC_dataset.npz\", 0.8)\n",
    "train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_FC_augmented_dataset.npz\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 116, 116])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    data_shape = X.shape\n",
    "    label_shape = y.shape\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LKA(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()        \n",
    "        attn = self.conv0(x)\n",
    "        attn = self.conv_spatial(attn)\n",
    "        attn = self.conv1(attn)\n",
    "\n",
    "        return u * attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.spatial_gating_unit = LKA(d_model)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shorcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        x = x + shorcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(dim)\n",
    "        self.attn = Attention(dim)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm2d(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=patch_size//2)\n",
    "        self.norm = nn.BatchNorm2d(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = self.norm(x)        \n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAN(nn.Module):\n",
    "    def __init__(self, img_size=224, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
    "                mlp_ratios=[4, 4, 4, 4], drop_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 depths=[3, 4, 6, 3], num_stages=4):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),\n",
    "                                            patch_size=7 if i == 0 else 3,\n",
    "                                            stride=4 if i == 0 else 2,\n",
    "                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n",
    "                                            embed_dim=embed_dims[i])\n",
    "\n",
    "            block = nn.ModuleList([Block(dim=embed_dims[i], mlp_ratio=mlp_ratios[i], drop=drop_rate)\n",
    "                for j in range(depths[i])])\n",
    "            norm = norm_layer(embed_dims[i])\n",
    "\n",
    "            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n",
    "            setattr(self, f\"block{i + 1}\", block)\n",
    "            setattr(self, f\"norm{i + 1}\", norm)\n",
    "\n",
    "        # classification head\n",
    "        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n",
    "            block = getattr(self, f\"block{i + 1}\")\n",
    "            norm = getattr(self, f\"norm{i + 1}\")\n",
    "            x, H, W = patch_embed(x)\n",
    "            for blk in block:\n",
    "                x = blk(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = norm(x)\n",
    "            if i != self.num_stages - 1:\n",
    "                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compile(self, dataloader:DataLoader, loss_fn, optimizer, lr=1e-2, device=None):\n",
    "        self.batch_size:int = dataloader.batch_size\n",
    "        for X, y in dataloader:\n",
    "            self.input_shape:tuple = X.shape\n",
    "            self.output_shape:tuple = y.shape\n",
    "            break\n",
    "        # self.build(self.input_shape)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def fit(self, dataloader:DataLoader, epochs:int=1, test_dataloader=None):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = size // self.batch_size\n",
    "        time_collection = []\n",
    "        loss_collection = []\n",
    "        correct_collection = []\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "            loss, correct = 0, 0\n",
    "            time_delta = 0\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                X = X.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # 计时\n",
    "                torch.cuda.synchronize()\n",
    "                time_start = time.time()\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = self.forward(X)\n",
    "                batch_loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计时结束\n",
    "                torch.cuda.synchronize()\n",
    "                time_end = time.time()\n",
    "\n",
    "                current = batch * self.batch_size + len(X)\n",
    "\n",
    "                batch_loss = batch_loss.item()\n",
    "                loss += batch_loss\n",
    "\n",
    "                batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                correct += batch_correct\n",
    "                batch_correct /= len(X)\n",
    "\n",
    "                batch_time = time_end - time_start\n",
    "                time_delta += batch_time\n",
    "                print(f\"\\r{batch+1}/{num_batches+1}  [{current:>3d}/{size:>3d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}% - {batch_time*1000:>0.3f}ms\", end = \"\", flush=True)\n",
    "            loss /= num_batches\n",
    "            correct /= size\n",
    "            print(f\"\\n-- Average loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}% - {time_delta/num_batches*1000:>0.3f}ms/batch\")\n",
    "            time_collection.append(time_delta)\n",
    "            loss_collection.append(loss)\n",
    "            correct_collection.append(correct)\n",
    "            if test_dataloader is not None:\n",
    "                self.test(test_dataloader)\n",
    "        print(\"\\n\", torchinfo.summary(self, input_size=self.input_shape))\n",
    "        return correct_collection, loss_collection, time_collection\n",
    "\n",
    "    def test(self, dataloader:DataLoader, return_preds=False):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        ys = []\n",
    "        preds = []\n",
    "        self.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                if return_preds:\n",
    "                    ys = np.hstack((ys, y.cpu()))\n",
    "                    preds = np.hstack((preds, pred.argmax(1).cpu()))\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        if return_preds:\n",
    "            return ys, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam\n",
    "correct, loss, timing = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "151/151  [9636/9636] - batch loss: 0.667334 - batch accuracy: 63.9% - 156.000ms\n",
      "-- Average loss: 0.686639 - Accuracy: 55.5% - 213.598ms/batch\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.667309 \n",
      "\n",
      "Epoch: 2/20\n",
      "151/151  [9636/9636] - batch loss: 0.560364 - batch accuracy: 83.3% - 121.997ms\n",
      "-- Average loss: 0.621962 - Accuracy: 71.4% - 174.319ms/batch\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.625314 \n",
      "\n",
      "Epoch: 3/20\n",
      "151/151  [9636/9636] - batch loss: 0.330613 - batch accuracy: 91.7% - 128.001ms\n",
      "-- Average loss: 0.491831 - Accuracy: 82.7% - 174.149ms/batch\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.544276 \n",
      "\n",
      "Epoch: 4/20\n",
      "151/151  [9636/9636] - batch loss: 0.079198 - batch accuracy: 100.0% - 120.000ms\n",
      "-- Average loss: 0.274282 - Accuracy: 91.3% - 174.565ms/batch\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.498872 \n",
      "\n",
      "Epoch: 5/20\n",
      "151/151  [9636/9636] - batch loss: 0.011338 - batch accuracy: 100.0% - 127.000ms\n",
      "-- Average loss: 0.082632 - Accuracy: 98.4% - 175.803ms/batch\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.573694 \n",
      "\n",
      "Epoch: 6/20\n",
      "151/151  [9636/9636] - batch loss: 0.003701 - batch accuracy: 100.0% - 127.000ms\n",
      "-- Average loss: 0.023007 - Accuracy: 99.9% - 174.081ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.614102 \n",
      "\n",
      "Epoch: 7/20\n",
      "151/151  [9636/9636] - batch loss: 0.001407 - batch accuracy: 100.0% - 127.998ms\n",
      "-- Average loss: 0.003856 - Accuracy: 100.0% - 174.486ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.661994 \n",
      "\n",
      "Epoch: 8/20\n",
      "151/151  [9636/9636] - batch loss: 0.000866 - batch accuracy: 100.0% - 133.999ms\n",
      "-- Average loss: 0.001918 - Accuracy: 100.0% - 175.863ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.699409 \n",
      "\n",
      "Epoch: 9/20\n",
      "151/151  [9636/9636] - batch loss: 0.000586 - batch accuracy: 100.0% - 117.999ms\n",
      "-- Average loss: 0.001228 - Accuracy: 100.0% - 175.962ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.730506 \n",
      "\n",
      "Epoch: 10/20\n",
      "151/151  [9636/9636] - batch loss: 0.000421 - batch accuracy: 100.0% - 126.998ms\n",
      "-- Average loss: 0.000861 - Accuracy: 100.0% - 175.846ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.757502 \n",
      "\n",
      "Epoch: 11/20\n",
      "151/151  [9636/9636] - batch loss: 0.000318 - batch accuracy: 100.0% - 120.002ms\n",
      "-- Average loss: 0.000636 - Accuracy: 100.0% - 175.617ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.781719 \n",
      "\n",
      "Epoch: 12/20\n",
      "151/151  [9636/9636] - batch loss: 0.000248 - batch accuracy: 100.0% - 126.999ms\n",
      "-- Average loss: 0.000488 - Accuracy: 100.0% - 175.105ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.803842 \n",
      "\n",
      "Epoch: 13/20\n",
      "151/151  [9636/9636] - batch loss: 0.000199 - batch accuracy: 100.0% - 123.998ms\n",
      "-- Average loss: 0.000385 - Accuracy: 100.0% - 175.795ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.824308 \n",
      "\n",
      "Epoch: 14/20\n",
      "151/151  [9636/9636] - batch loss: 0.000163 - batch accuracy: 100.0% - 122.560ms\n",
      "-- Average loss: 0.000311 - Accuracy: 100.0% - 177.213ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.843437 \n",
      "\n",
      "Epoch: 15/20\n",
      "151/151  [9636/9636] - batch loss: 0.000135 - batch accuracy: 100.0% - 122.999ms\n",
      "-- Average loss: 0.000255 - Accuracy: 100.0% - 177.113ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.861472 \n",
      "\n",
      "Epoch: 16/20\n",
      "151/151  [9636/9636] - batch loss: 0.000114 - batch accuracy: 100.0% - 125.998ms\n",
      "-- Average loss: 0.000212 - Accuracy: 100.0% - 178.645ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.878596 \n",
      "\n",
      "Epoch: 17/20\n",
      "151/151  [9636/9636] - batch loss: 0.000097 - batch accuracy: 100.0% - 132.990ms\n",
      "-- Average loss: 0.000179 - Accuracy: 100.0% - 178.072ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.894956 \n",
      "\n",
      "Epoch: 18/20\n",
      "151/151  [9636/9636] - batch loss: 0.000083 - batch accuracy: 100.0% - 123.998ms\n",
      "-- Average loss: 0.000152 - Accuracy: 100.0% - 177.156ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.910663 \n",
      "\n",
      "Epoch: 19/20\n",
      "151/151  [9636/9636] - batch loss: 0.000072 - batch accuracy: 100.0% - 125.999ms\n",
      "-- Average loss: 0.000130 - Accuracy: 100.0% - 177.195ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.925810 \n",
      "\n",
      "Epoch: 20/20\n",
      "151/151  [9636/9636] - batch loss: 0.000063 - batch accuracy: 100.0% - 123.000ms\n",
      "-- Average loss: 0.000112 - Accuracy: 100.0% - 180.356ms/batch\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.940469 \n",
      "\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VAN                                      --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "├─ModuleList: 1-4                        --                        --\n",
      "├─OverlapPatchEmbed: 1-5                 [64, 64, 29, 29]          --\n",
      "│    └─Conv2d: 2-1                       [64, 64, 29, 29]          3,200\n",
      "│    └─BatchNorm2d: 2-2                  [64, 64, 29, 29]          128\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Block: 2-3                        [64, 64, 29, 29]          --\n",
      "│    │    └─BatchNorm2d: 3-1             [64, 64, 29, 29]          128\n",
      "│    │    └─Attention: 3-2               [64, 64, 29, 29]          17,344\n",
      "│    │    └─BatchNorm2d: 3-3             [64, 64, 29, 29]          128\n",
      "│    │    └─Mlp: 3-4                     [64, 64, 29, 29]          35,648\n",
      "│    └─Block: 2-4                        [64, 64, 29, 29]          --\n",
      "│    │    └─BatchNorm2d: 3-5             [64, 64, 29, 29]          128\n",
      "│    │    └─Attention: 3-6               [64, 64, 29, 29]          17,344\n",
      "│    │    └─BatchNorm2d: 3-7             [64, 64, 29, 29]          128\n",
      "│    │    └─Mlp: 3-8                     [64, 64, 29, 29]          35,648\n",
      "│    └─Block: 2-5                        [64, 64, 29, 29]          --\n",
      "│    │    └─BatchNorm2d: 3-9             [64, 64, 29, 29]          128\n",
      "│    │    └─Attention: 3-10              [64, 64, 29, 29]          17,344\n",
      "│    │    └─BatchNorm2d: 3-11            [64, 64, 29, 29]          128\n",
      "│    │    └─Mlp: 3-12                    [64, 64, 29, 29]          35,648\n",
      "├─LayerNorm: 1-6                         [64, 841, 64]             128\n",
      "├─OverlapPatchEmbed: 1-7                 [64, 128, 15, 15]         --\n",
      "│    └─Conv2d: 2-6                       [64, 128, 15, 15]         73,856\n",
      "│    └─BatchNorm2d: 2-7                  [64, 128, 15, 15]         256\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "│    └─Block: 2-8                        [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-13            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-14              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-15            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-16                    [64, 128, 15, 15]         136,832\n",
      "│    └─Block: 2-9                        [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-17            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-18              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-19            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-20                    [64, 128, 15, 15]         136,832\n",
      "│    └─Block: 2-10                       [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-21            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-22              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-23            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-24                    [64, 128, 15, 15]         136,832\n",
      "│    └─Block: 2-11                       [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-25            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-26              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-27            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-28                    [64, 128, 15, 15]         136,832\n",
      "├─LayerNorm: 1-8                         [64, 225, 128]            256\n",
      "├─OverlapPatchEmbed: 1-9                 [64, 256, 8, 8]           --\n",
      "│    └─Conv2d: 2-12                      [64, 256, 8, 8]           295,168\n",
      "│    └─BatchNorm2d: 2-13                 [64, 256, 8, 8]           512\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Block: 2-14                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-29            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-30              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-31            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-32                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-15                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-33            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-34              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-35            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-36                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-16                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-37            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-38              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-39            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-40                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-17                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-41            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-42              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-43            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-44                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-18                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-45            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-46              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-47            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-48                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-19                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-49            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-50              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-51            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-52                    [64, 256, 8, 8]           535,808\n",
      "├─LayerNorm: 1-10                        [64, 64, 256]             512\n",
      "├─OverlapPatchEmbed: 1-11                [64, 512, 4, 4]           --\n",
      "│    └─Conv2d: 2-20                      [64, 512, 4, 4]           1,180,160\n",
      "│    └─BatchNorm2d: 2-21                 [64, 512, 4, 4]           1,024\n",
      "├─ModuleList: 1-4                        --                        --\n",
      "│    └─Block: 2-22                       [64, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-53            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Attention: 3-54              [64, 512, 4, 4]           826,880\n",
      "│    │    └─BatchNorm2d: 3-55            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Mlp: 3-56                    [64, 512, 4, 4]           2,120,192\n",
      "│    └─Block: 2-23                       [64, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-57            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Attention: 3-58              [64, 512, 4, 4]           826,880\n",
      "│    │    └─BatchNorm2d: 3-59            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Mlp: 3-60                    [64, 512, 4, 4]           2,120,192\n",
      "│    └─Block: 2-24                       [64, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-61            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Attention: 3-62              [64, 512, 4, 4]           826,880\n",
      "│    │    └─BatchNorm2d: 3-63            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Mlp: 3-64                    [64, 512, 4, 4]           2,120,192\n",
      "├─LayerNorm: 1-12                        [64, 16, 512]             1,024\n",
      "├─Linear: 1-13                           [64, 2]                   1,026\n",
      "==========================================================================================\n",
      "Total params: 15,872,770\n",
      "Trainable params: 15,872,770\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 51.06\n",
      "==========================================================================================\n",
      "Input size (MB): 3.44\n",
      "Forward/backward pass size (MB): 3437.79\n",
      "Params size (MB): 63.49\n",
      "Estimated Total Size (MB): 3504.73\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = VAN(data_shape[2], data_shape[1], 2)\n",
    "model.compile(train_dataloader, loss_fn, optimizer, lr, device)\n",
    "correct[\"RLKA\"], loss[\"RLKA\"], timing[\"RLKA\"] = model.fit(train_dataloader, epochs, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000103 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.940469 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.test(train_dataloader)\n",
    "model.test(test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
