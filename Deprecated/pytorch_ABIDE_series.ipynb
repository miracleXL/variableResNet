{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchinfo\n",
    "import time\n",
    "from multiprocessing import Process\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device:【cuda:None】\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU Device:【{}:{}】\".format(device.type, device.index))\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data:np.ndarray, labels:np.ndarray, transform=ToTensor(), \n",
    "    target_transform=Lambda(lambda y: torch.zeros(2, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))):\n",
    "        self.data:torch.Tensor = torch.from_numpy(data)\n",
    "        self.labels:torch.Tensor = torch.from_numpy(labels)\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "        # self.transform = transform\n",
    "        # self.target_transform = target_transform\n",
    "        # self.shuffle()\n",
    "    \n",
    "    def shuffle(self, seed=None):\n",
    "        '\\n        seed(self, seed=None)\\n\\n        Reseed a legacy MT19937 BitGenerator\\n        '\n",
    "        self.shuffle_seed = np.random.randint(1, 65535) if seed is None else seed\n",
    "        print(f\"随机种子：{self.shuffle_seed}\")\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.data)\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx, 0]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=\"dataset.npz\", train_percent=0.8) -> tuple:\n",
    "    with np.load(path) as dataset:\n",
    "        full_data = dataset[\"data\"].astype(np.float32)\n",
    "        full_labels = dataset[\"labels\"].astype(np.int64)\n",
    "    train_size = int(full_data.shape[0]*train_percent)\n",
    "    test_size = full_data.shape[0]-train_size\n",
    "    seed = np.random.randint(1, 65535) # 35468\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_data)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_labels)\n",
    "    train_data, test_data = full_data[:train_size], full_data[train_size:]\n",
    "    train_labels, test_labels = full_labels[:train_size], full_labels[train_size:]\n",
    "    print(f\"训练集大小：{train_size}\", f\"测试集大小：{test_size}\", f\"随机种子：{seed}\")\n",
    "    train_dataset = CustomDataset(train_data, train_labels)\n",
    "    test_dataset = CustomDataset(test_data, test_labels)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：9636 测试集大小：2409 随机种子：35468\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_augmented_dataset.npz\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, L, H]: torch.Size([64, 60, 116])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, L, H]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimlpeLSTMCNN(nn.Module):\n",
    "    def __init__(self, device=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.device = device\n",
    "        self.__built = False\n",
    "    \n",
    "    def build(self, input_shape:tuple):\n",
    "        # resolve output shape in model summary\n",
    "        self.lstm = nn.LSTM(116, 116, 2, batch_first=True)\n",
    "        self.conv = nn.Conv1d(input_shape[1], 1, 3, padding=\"same\")\n",
    "        self.activation = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.liner = nn.LazyLinear(2)\n",
    "        self.__built = True\n",
    "\n",
    "    def forward(self, x:np.ndarray):\n",
    "        if not self.__built:\n",
    "            raise RuntimeWarning(\"模型未完成编译！\")\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.liner(x)\n",
    "        return x\n",
    "\n",
    "    def compile(self, dataloader:DataLoader, loss_fn, optimizer, lr=1e-2):\n",
    "        self.batch_size:int = dataloader.batch_size\n",
    "        for X, y in dataloader:\n",
    "            self.input_shape:tuple = X.shape\n",
    "            self.output_shape:tuple = y.shape\n",
    "            break\n",
    "        self.build(self.input_shape)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def fit(self, dataloader:DataLoader, epochs:int=1):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = size // self.batch_size\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "            loss, correct = 0, 0\n",
    "            time_start = time.time()\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                # time_batch_start = time.time()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = self.forward(X)\n",
    "                batch_loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                current = batch * self.batch_size + len(X)\n",
    "\n",
    "                batch_loss = batch_loss.item()\n",
    "                loss += batch_loss\n",
    "                batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                correct += batch_correct\n",
    "\n",
    "                batch_correct /= len(X)\n",
    "                # time_batch_end = time.time()\n",
    "                time_end = time.time()\n",
    "                print(f\"\\r{batch+1}/{num_batches+1}  [{current:>3d}/{size:>3d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}% - {(time_end-time_start)/num_batches*1000:>0.3f}ms/batch\", end = \"\", flush=True)\n",
    "            loss /= num_batches\n",
    "            correct /= size\n",
    "            time_end = time.time()\n",
    "            print(f\"\\n-- Average loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}% - {(time_end-time_start)*1000:>0.3f}ms\")\n",
    "        print(\"\\n\", torchinfo.summary(self, input_size=self.input_shape))\n",
    "\n",
    "    def test(self, dataloader:DataLoader):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        self.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_channels:int=None, kernel_size=5, device=None, *args, **wargs):\n",
    "        super().__init__(*args, **wargs)\n",
    "        self.__built = False\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        in_channels = input_shape[1]\n",
    "        out_channels = in_channels if self.out_channels is None else in_channels\n",
    "        # resolve output shape in model summary\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, self.kernel_size, padding=\"same\")\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm1d(in_channels)\n",
    "        self.downconv = nn.Conv1d(in_channels, out_channels, self.kernel_size, padding=\"same\")\n",
    "        self.downbn = nn.BatchNorm1d(in_channels)\n",
    "        self.output_shape = input_shape\n",
    "        self.__built = True\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, inputs:np.ndarray):\n",
    "        x:np.ndarray = inputs\n",
    "        fx:np.ndarray = x\n",
    "        fx = self.conv(fx)\n",
    "        fx = self.bn(fx)\n",
    "        if fx.shape[-1] != x.shape[-1]:\n",
    "            x = self.downconv(x)\n",
    "            x = self.downbn(x)\n",
    "        try:\n",
    "            return fx + x\n",
    "        except:\n",
    "            raise RuntimeError(x.shape, fx.shape, inputs.shape)\n",
    "    \n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self, device=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__built = False\n",
    "        self.device = device\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.lstm = nn.LSTM(116, 116, 2, batch_first=True)\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks:nn.ModuleList = nn.ModuleList([ResBlock(), ResBlock(), ResBlock()])\n",
    "        self.blocks_1:nn.ModuleList = nn.ModuleList([ResBlock(1)])\n",
    "        for block in self.blocks:\n",
    "            block.build(input_shape)\n",
    "            block.to(self.device)\n",
    "        for block in self.blocks_1:\n",
    "            block.build(input_shape)\n",
    "            block.to(self.device)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(np.prod(input_shape[1:]), 2)\n",
    "        self.__built = True\n",
    "\n",
    "    def compile(self, dataloader:DataLoader, loss_fn, optimizer, lr=1e-2):\n",
    "        self.batch_size:int = dataloader.batch_size\n",
    "        for X, y in dataloader:\n",
    "            self.input_shape:tuple = X.shape\n",
    "            self.output_shape:tuple = y.shape\n",
    "            break\n",
    "        self.build(self.input_shape)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(self.parameters(), lr=lr)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self, dataloader:DataLoader, epochs:int=1):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = size // self.batch_size\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "            loss, correct = 0, 0\n",
    "            time_start = time.time()\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                # time_batch_start = time.time()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = self.forward(X)\n",
    "                batch_loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                current = batch * self.batch_size + len(X)\n",
    "\n",
    "                batch_loss = batch_loss.item()\n",
    "                loss += batch_loss\n",
    "                batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                correct += batch_correct\n",
    "\n",
    "                batch_correct /= len(X)\n",
    "                # time_batch_end = time.time()\n",
    "                time_end = time.time()\n",
    "                print(f\"\\r{batch+1}/{num_batches+1}  [{current:>3d}/{size:>3d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}% - {(time_end-time_start)/num_batches*1000:>0.3f}ms/batch\", end = \"\", flush=True)\n",
    "            loss /= num_batches\n",
    "            correct /= size\n",
    "            time_end = time.time()\n",
    "            print(f\"\\n-- Average loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}% - {(time_end-time_start)*1000:>0.3f}ms\")\n",
    "        print(\"\\n\", torchinfo.summary(self, input_size=self.input_shape))\n",
    "\n",
    "    def test(self, dataloader:DataLoader):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        self.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, device=None, copy_block=False, cache=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__blocks_num:int = 1\n",
    "        self.__frozen_blocks_num:int = 0\n",
    "        self.__built:bool = False\n",
    "        self.cache:bool = cache\n",
    "        self.__cache:list(torch.TensorType) = []\n",
    "        self.device:torch.DeviceObjType = device\n",
    "        self.copy_block:bool = copy_block\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.lstm = nn.LSTM(116, 116, 2, batch_first=True)\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks:nn.ModuleList = nn.ModuleList([ResBlock()])\n",
    "        self.blocks1:nn.ModuleList = nn.ModuleList([ResBlock(1)])\n",
    "        for block in self.blocks:\n",
    "            block.build(input_shape)\n",
    "            block.to(self.device)\n",
    "        for block in self.blocks1:\n",
    "            block.build(input_shape)\n",
    "            block.to(self.device)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(np.prod(input_shape[1:]), 2)\n",
    "        self.__built = True\n",
    "\n",
    "    def compile(self, dataloader:DataLoader, loss_fn, optimizer, lr=1e-2):\n",
    "        self.batch_size:int = dataloader.batch_size\n",
    "        for X, y in dataloader:\n",
    "            self.input_shape:tuple = X.shape\n",
    "            self.output_shape:tuple = y.shape\n",
    "            break\n",
    "        self.build(self.input_shape)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "        if self.cache:\n",
    "            for i in range(self.__frozen_blocks_num, self.__blocks_num):\n",
    "                x = self.blocks[i](x)\n",
    "        else:\n",
    "            for i in range(self.__blocks_num):\n",
    "                x = self.blocks[i](x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def getBlocksNum(self):\n",
    "        return self.__blocks_num\n",
    "    \n",
    "    def freeze(self, block:ResBlock):\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def __forward_cache(self, block):\n",
    "        if self.cache:\n",
    "            for batch, X in enumerate(self.__cache):\n",
    "                self.__cache[batch] = block(X.to(self.device))\n",
    "\n",
    "    def addNewBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"Adding new block...\")\n",
    "        newBlock = ResBlock()\n",
    "        last_block:ResBlock = self.blocks[-1]\n",
    "        newBlock.build(last_block.output_shape)\n",
    "        self.blocks.append(newBlock)\n",
    "        newBlock.to(self.device)\n",
    "        self.__blocks_num += 1\n",
    "        self.freeze(last_block)\n",
    "        self.__frozen_blocks_num += 1\n",
    "        self.__forward_cache(last_block)\n",
    "        print(\"Success!\")\n",
    "\n",
    "    def copyLastBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"Copying last block...\")\n",
    "        newBlock = ResBlock()\n",
    "        last_block:ResBlock = self.blocks[-1]\n",
    "        newBlock.build(last_block.output_shape)\n",
    "        if last_block.input_shape == last_block.output_shape:\n",
    "            newBlock.load_state_dict(last_block.state_dict())\n",
    "        else:\n",
    "            print(\"Copy failed: shape different with last block\")\n",
    "        self.blocks.append(newBlock)\n",
    "        newBlock.to(self.device)\n",
    "        self.__blocks_num += 1\n",
    "        self.freeze(last_block)\n",
    "        self.__frozen_blocks_num += 1\n",
    "        self.__forward_cache(last_block)\n",
    "        print(\"Success!\")\n",
    "\n",
    "    def fit(self, dataloader:DataLoader, epochs:int=1):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = size // self.batch_size\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "            if epoch and epoch%3 == 0:\n",
    "                if self.copy_block:\n",
    "                    self.copyLastBlock()\n",
    "                else:\n",
    "                    self.addNewBlock()\n",
    "            loss, correct = 0, 0\n",
    "            torch.cuda.synchronize()\n",
    "            time_start = time.time()\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                # time_batch_start = time.time()\n",
    "                if self.cache:\n",
    "                    if epoch == 0:\n",
    "                        self.__cache.append(X)\n",
    "                    X = self.__cache[batch].to(device)\n",
    "                else:\n",
    "                    X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = self.forward(X)\n",
    "                batch_loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                current = batch * self.batch_size + len(X)\n",
    "\n",
    "                batch_loss = batch_loss.item()\n",
    "                loss += batch_loss\n",
    "                batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                correct += batch_correct\n",
    "\n",
    "                batch_correct /= len(X)\n",
    "                # time_batch_end = time.time()\n",
    "                torch.cuda.synchronize()\n",
    "                time_end = time.time()\n",
    "                print(f\"\\r{batch+1}/{num_batches+1}  [{current:>3d}/{size:>3d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}% - {(time_end-time_start)/num_batches*1000:>0.3f}ms/batch\", end = \"\", flush=True)\n",
    "            loss /= num_batches\n",
    "            correct /= size\n",
    "            torch.cuda.synchronize()\n",
    "            time_end = time.time()\n",
    "            print(f\"\\n-- Average loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}% - {(time_end-time_start)*1000:>0.3f}ms\")\n",
    "        print(\"\\n\", torchinfo.summary(self, input_size=self.input_shape))\n",
    "\n",
    "    def test(self, dataloader:DataLoader):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        self.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 9\n",
    "loss_fn = nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9\n",
      "151/151  [9636/9636] - batch loss: 0.632798 - batch accuracy: 66.7% - 35.031ms/batch\n",
      "-- Average loss: 2.837251 - Accuracy: 52.4% - 5255.603ms\n",
      "Epoch: 2/9\n",
      "151/151  [9636/9636] - batch loss: 0.412600 - batch accuracy: 75.0% - 18.289ms/batch\n",
      "-- Average loss: 0.506696 - Accuracy: 73.7% - 2744.315ms\n",
      "Epoch: 3/9\n",
      "151/151  [9636/9636] - batch loss: 0.408411 - batch accuracy: 86.1% - 18.336ms/batch\n",
      "-- Average loss: 0.307631 - Accuracy: 86.2% - 2751.330ms\n",
      "Epoch: 4/9\n",
      "151/151  [9636/9636] - batch loss: 0.086233 - batch accuracy: 94.4% - 18.380ms/batch\n",
      "-- Average loss: 0.202394 - Accuracy: 92.0% - 2757.979ms\n",
      "Epoch: 5/9\n",
      "151/151  [9636/9636] - batch loss: 0.265240 - batch accuracy: 88.9% - 18.207ms/batch\n",
      "-- Average loss: 0.161910 - Accuracy: 93.9% - 2732.985ms\n",
      "Epoch: 6/9\n",
      "151/151  [9636/9636] - batch loss: 0.049649 - batch accuracy: 97.2% - 18.327ms/batchh\n",
      "-- Average loss: 0.138168 - Accuracy: 94.7% - 2750.000ms\n",
      "Epoch: 7/9\n",
      "151/151  [9636/9636] - batch loss: 0.002106 - batch accuracy: 100.0% - 18.503ms/batch\n",
      "-- Average loss: 0.090021 - Accuracy: 96.6% - 2776.411ms\n",
      "Epoch: 8/9\n",
      "151/151  [9636/9636] - batch loss: 0.105024 - batch accuracy: 94.4% - 18.518ms/batchh\n",
      "-- Average loss: 0.085316 - Accuracy: 97.0% - 2778.661ms\n",
      "Epoch: 9/9\n",
      "151/151  [9636/9636] - batch loss: 0.015276 - batch accuracy: 100.0% - 18.601ms/batch\n",
      "-- Average loss: 0.112510 - Accuracy: 95.8% - 2792.203ms\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "SimpleResNet                             --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─LSTM: 1-3                              [64, 60, 116]             217,152\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─ResBlock: 2-1                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-1                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-2             [64, 60, 116]             120\n",
      "│    └─ResBlock: 2-2                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-3                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-4             [64, 60, 116]             120\n",
      "│    └─ResBlock: 2-3                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-5                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-6             [64, 60, 116]             120\n",
      "├─Flatten: 1-4                           [64, 6960]                --\n",
      "├─Linear: 1-5                            [64, 2]                   13,922\n",
      "==========================================================================================\n",
      "Total params: 285,614\n",
      "Trainable params: 285,614\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.24\n",
      "==========================================================================================\n",
      "Input size (MB): 1.78\n",
      "Forward/backward pass size (MB): 24.95\n",
      "Params size (MB): 1.14\n",
      "Estimated Total Size (MB): 27.87\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "simple_model = SimpleResNet(device)\n",
    "simple_model.compile(train_dataloader, loss_fn=loss_fn, optimizer=optimizer, lr=lr)\n",
    "simple_model.fit(train_dataloader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9\n",
      "151/151  [9636/9636] - batch loss: 1.376981 - batch accuracy: 47.2% - 15.242ms/batch\n",
      "-- Average loss: 2.090946 - Accuracy: 57.4% - 2287.260ms\n",
      "Epoch: 2/9\n",
      "151/151  [9636/9636] - batch loss: 0.616090 - batch accuracy: 83.3% - 15.801ms/batch\n",
      "-- Average loss: 0.603029 - Accuracy: 77.4% - 2371.142ms\n",
      "Epoch: 3/9\n",
      "151/151  [9636/9636] - batch loss: 0.692257 - batch accuracy: 75.0% - 16.062ms/batch\n",
      "-- Average loss: 0.326872 - Accuracy: 86.3% - 2410.265ms\n",
      "Epoch: 4/9\n",
      "----------\n",
      "Adding new block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.740456 - batch accuracy: 80.6% - 17.104ms/batch\n",
      "-- Average loss: 1.951129 - Accuracy: 75.2% - 2566.612ms\n",
      "Epoch: 5/9\n",
      "151/151  [9636/9636] - batch loss: 0.450994 - batch accuracy: 83.3% - 17.027ms/batch\n",
      "-- Average loss: 0.918521 - Accuracy: 82.0% - 2556.100ms\n",
      "Epoch: 6/9\n",
      "151/151  [9636/9636] - batch loss: 1.029123 - batch accuracy: 77.8% - 16.755ms/batch\n",
      "-- Average loss: 0.935290 - Accuracy: 82.1% - 2513.190ms\n",
      "Epoch: 7/9\n",
      "----------\n",
      "Adding new block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.544326 - batch accuracy: 88.9% - 17.058ms/batch\n",
      "-- Average loss: 2.004375 - Accuracy: 80.0% - 2559.715ms\n",
      "Epoch: 8/9\n",
      "151/151  [9636/9636] - batch loss: 1.848735 - batch accuracy: 83.3% - 18.058ms/batch\n",
      "-- Average loss: 1.205916 - Accuracy: 82.8% - 2709.634ms\n",
      "Epoch: 9/9\n",
      "151/151  [9636/9636] - batch loss: 10.332379 - batch accuracy: 61.1% - 17.173ms/batch\n",
      "-- Average loss: 1.177825 - Accuracy: 84.1% - 2577.969ms\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MyResNet                                 --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─LSTM: 1-3                              [64, 60, 116]             217,152\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─ResBlock: 2-1                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-1                  [64, 60, 116]             (18,060)\n",
      "│    │    └─BatchNorm1d: 3-2             [64, 60, 116]             (120)\n",
      "│    └─ResBlock: 2-2                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-3                  [64, 60, 116]             (18,060)\n",
      "│    │    └─BatchNorm1d: 3-4             [64, 60, 116]             (120)\n",
      "│    └─ResBlock: 2-3                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-5                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-6             [64, 60, 116]             120\n",
      "├─Flatten: 1-4                           [64, 6960]                --\n",
      "├─Linear: 1-5                            [64, 2]                   13,922\n",
      "==========================================================================================\n",
      "Total params: 285,614\n",
      "Trainable params: 249,254\n",
      "Non-trainable params: 36,360\n",
      "Total mult-adds (G): 1.24\n",
      "==========================================================================================\n",
      "Input size (MB): 1.78\n",
      "Forward/backward pass size (MB): 24.95\n",
      "Params size (MB): 1.14\n",
      "Estimated Total Size (MB): 27.87\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = MyResNet(device)\n",
    "model.compile(train_dataloader, loss_fn=loss_fn, optimizer=optimizer, lr=lr)\n",
    "model.fit(train_dataloader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9\n",
      "151/151  [9636/9636] - batch loss: 0.569411 - batch accuracy: 69.4% - 15.746ms/batch\n",
      "-- Average loss: 2.299541 - Accuracy: 54.7% - 2362.936ms\n",
      "Epoch: 2/9\n",
      "151/151  [9636/9636] - batch loss: 0.835074 - batch accuracy: 72.2% - 16.293ms/batch\n",
      "-- Average loss: 0.481192 - Accuracy: 78.4% - 2444.907ms\n",
      "Epoch: 3/9\n",
      "151/151  [9636/9636] - batch loss: 0.584027 - batch accuracy: 86.1% - 16.102ms/batch\n",
      "-- Average loss: 0.378450 - Accuracy: 85.4% - 2416.286ms\n",
      "Epoch: 4/9\n",
      "----------\n",
      "Adding new block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 2.698141 - batch accuracy: 58.3% - 15.459ms/batchh\n",
      "-- Average loss: 5.281126 - Accuracy: 60.9% - 2319.817ms\n",
      "Epoch: 5/9\n",
      "151/151  [9636/9636] - batch loss: 0.597353 - batch accuracy: 83.3% - 15.569ms/batch\n",
      "-- Average loss: 1.901757 - Accuracy: 72.3% - 2336.408ms\n",
      "Epoch: 6/9\n",
      "151/151  [9636/9636] - batch loss: 3.392251 - batch accuracy: 58.3% - 15.391ms/batch\n",
      "-- Average loss: 1.094381 - Accuracy: 81.1% - 2309.643ms\n",
      "Epoch: 7/9\n",
      "----------\n",
      "Adding new block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.950613 - batch accuracy: 83.3% - 15.460ms/batch\n",
      "-- Average loss: 1.090930 - Accuracy: 79.4% - 2319.952ms\n",
      "Epoch: 8/9\n",
      "151/151  [9636/9636] - batch loss: 4.529712 - batch accuracy: 55.6% - 15.848ms/batch\n",
      "-- Average loss: 0.936213 - Accuracy: 82.6% - 2378.159ms\n",
      "Epoch: 9/9\n",
      "151/151  [9636/9636] - batch loss: 0.045612 - batch accuracy: 100.0% - 15.522ms/batch\n",
      "-- Average loss: 0.817979 - Accuracy: 84.8% - 2329.240ms\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MyResNet                                 --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─LSTM: 1-3                              [64, 60, 116]             217,152\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─ResBlock: 2-1                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-1                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-2             [64, 60, 116]             120\n",
      "├─Flatten: 1-4                           [64, 6960]                --\n",
      "├─Linear: 1-5                            [64, 2]                   13,922\n",
      "==========================================================================================\n",
      "Total params: 249,254\n",
      "Trainable params: 249,254\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 968.84\n",
      "==========================================================================================\n",
      "Input size (MB): 1.78\n",
      "Forward/backward pass size (MB): 10.69\n",
      "Params size (MB): 1.00\n",
      "Estimated Total Size (MB): 13.47\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "cache_model = MyResNet(device, cache=True)\n",
    "cache_model.compile(train_dataloader, loss_fn=loss_fn, optimizer=optimizer, lr=lr)\n",
    "cache_model.fit(train_dataloader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9\n",
      "151/151  [9636/9636] - batch loss: 0.654439 - batch accuracy: 63.9% - 16.772ms/batch\n",
      "-- Average loss: 3.038122 - Accuracy: 52.2% - 2516.848ms\n",
      "Epoch: 2/9\n",
      "151/151  [9636/9636] - batch loss: 0.547437 - batch accuracy: 72.2% - 16.535ms/batch\n",
      "-- Average loss: 0.739187 - Accuracy: 56.0% - 2480.309ms\n",
      "Epoch: 3/9\n",
      "151/151  [9636/9636] - batch loss: 0.572166 - batch accuracy: 72.2% - 16.055ms/batch\n",
      "-- Average loss: 0.504303 - Accuracy: 74.6% - 2409.187ms\n",
      "Epoch: 4/9\n",
      "----------\n",
      "Copying last block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.407271 - batch accuracy: 77.8% - 16.889ms/batch\n",
      "-- Average loss: 0.930404 - Accuracy: 72.1% - 2534.419ms\n",
      "Epoch: 5/9\n",
      "151/151  [9636/9636] - batch loss: 1.949872 - batch accuracy: 55.6% - 17.007ms/batch\n",
      "-- Average loss: 0.667843 - Accuracy: 78.2% - 2552.048ms\n",
      "Epoch: 6/9\n",
      "151/151  [9636/9636] - batch loss: 0.276107 - batch accuracy: 86.1% - 17.152ms/batch\n",
      "-- Average loss: 0.629826 - Accuracy: 81.0% - 2574.816ms\n",
      "Epoch: 7/9\n",
      "----------\n",
      "Copying last block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.196920 - batch accuracy: 91.7% - 17.366ms/batch\n",
      "-- Average loss: 0.408803 - Accuracy: 85.2% - 2604.849ms\n",
      "Epoch: 8/9\n",
      "151/151  [9636/9636] - batch loss: 0.191501 - batch accuracy: 94.4% - 17.213ms/batch\n",
      "-- Average loss: 0.242841 - Accuracy: 88.9% - 2583.011ms\n",
      "Epoch: 9/9\n",
      "151/151  [9636/9636] - batch loss: 0.124004 - batch accuracy: 100.0% - 17.385ms/batch\n",
      "-- Average loss: 0.261935 - Accuracy: 88.7% - 2608.753ms\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MyResNet                                 --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─LSTM: 1-3                              [64, 60, 116]             217,152\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─ResBlock: 2-1                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-1                  [64, 60, 116]             (18,060)\n",
      "│    │    └─BatchNorm1d: 3-2             [64, 60, 116]             (120)\n",
      "│    └─ResBlock: 2-2                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-3                  [64, 60, 116]             (18,060)\n",
      "│    │    └─BatchNorm1d: 3-4             [64, 60, 116]             (120)\n",
      "│    └─ResBlock: 2-3                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-5                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-6             [64, 60, 116]             120\n",
      "├─Flatten: 1-4                           [64, 6960]                --\n",
      "├─Linear: 1-5                            [64, 2]                   13,922\n",
      "==========================================================================================\n",
      "Total params: 285,614\n",
      "Trainable params: 249,254\n",
      "Non-trainable params: 36,360\n",
      "Total mult-adds (G): 1.24\n",
      "==========================================================================================\n",
      "Input size (MB): 1.78\n",
      "Forward/backward pass size (MB): 24.95\n",
      "Params size (MB): 1.14\n",
      "Estimated Total Size (MB): 27.87\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "copy_nocache_model = MyResNet(device, copy_block=True, cache=False)\n",
    "copy_nocache_model.compile(train_dataloader, loss_fn=loss_fn, optimizer=optimizer, lr=lr)\n",
    "copy_nocache_model.fit(train_dataloader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9\n",
      "151/151  [9636/9636] - batch loss: 0.784736 - batch accuracy: 55.6% - 16.006ms/batch\n",
      "-- Average loss: 2.576262 - Accuracy: 55.1% - 2402.967ms\n",
      "Epoch: 2/9\n",
      "151/151  [9636/9636] - batch loss: 0.493238 - batch accuracy: 72.2% - 16.317ms/batch\n",
      "-- Average loss: 0.598163 - Accuracy: 76.5% - 2448.535ms\n",
      "Epoch: 3/9\n",
      "151/151  [9636/9636] - batch loss: 0.340224 - batch accuracy: 88.9% - 16.132ms/batch\n",
      "-- Average loss: 0.337614 - Accuracy: 85.7% - 2420.732ms\n",
      "Epoch: 4/9\n",
      "----------\n",
      "Copying last block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.455099 - batch accuracy: 88.9% - 15.658ms/batchh\n",
      "-- Average loss: 0.221921 - Accuracy: 91.3% - 2348.665ms\n",
      "Epoch: 5/9\n",
      "151/151  [9636/9636] - batch loss: 0.015366 - batch accuracy: 100.0% - 15.640ms/batch\n",
      "-- Average loss: 0.150251 - Accuracy: 94.3% - 2347.071ms\n",
      "Epoch: 6/9\n",
      "151/151  [9636/9636] - batch loss: 0.280999 - batch accuracy: 91.7% - 15.573ms/batchh\n",
      "-- Average loss: 0.119422 - Accuracy: 95.6% - 2336.007ms\n",
      "Epoch: 7/9\n",
      "----------\n",
      "Copying last block...\n",
      "Success!\n",
      "151/151  [9636/9636] - batch loss: 0.030310 - batch accuracy: 100.0% - 15.532ms/batch\n",
      "-- Average loss: 0.139338 - Accuracy: 95.0% - 2332.806ms\n",
      "Epoch: 8/9\n",
      "151/151  [9636/9636] - batch loss: 0.119742 - batch accuracy: 94.4% - 15.862ms/batchh\n",
      "-- Average loss: 0.126166 - Accuracy: 95.7% - 2380.339ms\n",
      "Epoch: 9/9\n",
      "151/151  [9636/9636] - batch loss: 0.002591 - batch accuracy: 100.0% - 15.669ms/batch\n",
      "-- Average loss: 0.085141 - Accuracy: 97.0% - 2356.326ms\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "MyResNet                                 --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─LSTM: 1-3                              [64, 60, 116]             217,152\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─ResBlock: 2-1                     [64, 60, 116]             --\n",
      "│    │    └─Conv1d: 3-1                  [64, 60, 116]             18,060\n",
      "│    │    └─BatchNorm1d: 3-2             [64, 60, 116]             120\n",
      "├─Flatten: 1-4                           [64, 6960]                --\n",
      "├─Linear: 1-5                            [64, 2]                   13,922\n",
      "==========================================================================================\n",
      "Total params: 249,254\n",
      "Trainable params: 249,254\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 968.84\n",
      "==========================================================================================\n",
      "Input size (MB): 1.78\n",
      "Forward/backward pass size (MB): 10.69\n",
      "Params size (MB): 1.00\n",
      "Estimated Total Size (MB): 13.47\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "copy_model = MyResNet(device, copy_block=True, cache=True)\n",
    "copy_model.compile(train_dataloader, loss_fn=loss_fn, optimizer=optimizer, lr=lr)\n",
    "copy_model.fit(train_dataloader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.181359 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 2.275847 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 1.053976 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.333633 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.199079 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_model.test(test_dataloader)\n",
    "model.test(test_dataloader)\n",
    "cache_model.test(test_dataloader)\n",
    "copy_nocache_model.test(test_dataloader)\n",
    "copy_model.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.069871 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 1.396245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.494309 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.191386 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.078086 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "simple_model.test(train_dataloader)\n",
    "model.test(train_dataloader)\n",
    "cache_model.test(train_dataloader)\n",
    "copy_nocache_model.test(train_dataloader)\n",
    "copy_model.test(train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
