{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Model, layers, Sequential\n",
    "from threading import Thread\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/fit/\" + current_time\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "# test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "# train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "# test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images dataset\n",
    "def load_dataset(name:str=\"mnist\", size:int=None):\n",
    "    if name == \"mnist\":\n",
    "        (train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
    "    elif name == \"cifar10\":\n",
    "        (train_x, train_y), (test_x, test_y) = keras.datasets.cifar10.load_data()\n",
    "    train_x, test_x = train_x/255.0, test_x/255.0\n",
    "\n",
    "    if size:\n",
    "        train_x = train_x[:size][..., tf.newaxis].astype(\"float32\")\n",
    "        test_x = test_x[:size][..., tf.newaxis].astype(\"float32\")\n",
    "        train_y, test_y = train_y[:size], test_y[:size]\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=30\n",
    "(train_x, train_y), (test_x, test_y) = load_dataset(\"cifar10\",size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, *args, **wargs):\n",
    "        super().__init__(*args, **wargs)\n",
    "        self.conv = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.downconv = layers.Conv2D(64, 1, padding=\"same\")\n",
    "        self.downbn = layers.BatchNormalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # resolve output shape in model summary\n",
    "        input_layer = layers.Input(shape=input_shape[1:], batch_size=input_shape[0])\n",
    "        self.call(input_layer)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs:np.ndarray):\n",
    "        x:np.ndarray = inputs\n",
    "        fx:np.ndarray = x\n",
    "        try:\n",
    "            fx = self.conv(fx)\n",
    "        except Exception as e:\n",
    "            tf.print(e)\n",
    "            raise RuntimeError(\"conv error in \",self.name,x.shape, fx.shape, inputs.shape)\n",
    "        fx = self.bn(fx)\n",
    "        if fx.shape[-1] != x.shape[-1]:\n",
    "            x = self.downconv(x)\n",
    "            x = self.downbn(x)\n",
    "        try:\n",
    "            return fx + x\n",
    "        except:\n",
    "            raise RuntimeError(x.shape, fx.shape, inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BothResNet(Model):\n",
    "    def __init__(self, use_cache=True, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__blocks_num = tf.Variable(1,dtype=tf.int8, trainable=False)\n",
    "        self.__frozen_blocks_num = tf.Variable(0,dtype=tf.int8, trainable=False)\n",
    "        self.use_cache = tf.constant(True) if use_cache else tf.constant(False)\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks = [ResBlock(name=\"res_block0\")]\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(10)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # resolve output shape in model summary\n",
    "        input_layer = layers.Input(shape=input_shape[1:], batch_size=input_shape[0])\n",
    "        self.call(input_layer)\n",
    "        return super().build(input_shape)\n",
    "    \n",
    "    def __use_last(self, x:tf.Tensor, training=None) -> tf.Tensor:\n",
    "        return self.blocks[-1](x, training=training)\n",
    "    \n",
    "    def __use_all(self, x:tf.Tensor, training=None) -> tf.Tensor:\n",
    "        raise RuntimeError(\"run the wrong function\")\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def call(self, x=None, training=None, mask=None):\n",
    "        # 只用最后一个残差块进行训练\n",
    "        x = tf.cond(self.use_cache, lambda: self.__use_last(x, training), lambda: self.__use_all(x, training))\n",
    "        x = self.flatten(x, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        return x\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def getBlocksNum(self) -> tf.int8:\n",
    "        return self.__blocks_num\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def getLastFrozenBlock(self) -> ResBlock:\n",
    "        index = self.__frozen_blocks_num.numpy()-1\n",
    "        block = self.blocks[index]\n",
    "        return block\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def freezeBlock(self):\n",
    "        if self.__frozen_blocks_num < self.__blocks_num:\n",
    "            index = self.__frozen_blocks_num.numpy()\n",
    "            block = self.blocks[index]\n",
    "            block.trainable = False\n",
    "            self.__frozen_blocks_num.assign_add(1)\n",
    "            print(\"freeze block:\", self.blocks[index].name, \", total frozen blocks:\", index+1)\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def addNewBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"add new block\")\n",
    "        i = self.__blocks_num.numpy()\n",
    "        newBlock = ResBlock(name=\"res_block\"+str(i))\n",
    "        newBlock(self.blocks[-1].output)\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num.assign_add(1)\n",
    "        print(f\"this is the {i+1} added blocks, block name: {self.blocks[i].name}\")\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def copyLastBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"copy last block\")\n",
    "        newBlock = ResBlock(name=\"res_block\"+str(self.__blocks_num.numpy()))\n",
    "        last_block:ResBlock = self.blocks[-1]\n",
    "        newBlock(last_block.output)\n",
    "        if last_block.input_shape == last_block.output_shape:\n",
    "            newBlock.set_weights(last_block.get_weights())\n",
    "        else:\n",
    "            print(\"copy failed: shape different with last block\")\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__blocks_num = tf.Variable(1,dtype=tf.int8, trainable=False)\n",
    "        self.__frozen_blocks_num = tf.Variable(0,dtype=tf.int8, trainable=False)\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks = [ResBlock(name=\"res_block0\")]\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(10)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # resolve output shape in model summary\n",
    "        input_layer = layers.Input(shape=input_shape[1:], batch_size=input_shape[0])\n",
    "        self.call(input_layer)\n",
    "        return super().build(input_shape)\n",
    "    \n",
    "    def call(self, x=None, training=None, mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "        x = self.flatten(x, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        return x\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def getBlocksNum(self) -> tf.int8:\n",
    "        return self.__blocks_num\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def getLastFrozenBlock(self) -> ResBlock:\n",
    "        index = self.__frozen_blocks_num.numpy()-1\n",
    "        block = self.blocks[index]\n",
    "        return block\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def freezeBlock(self):\n",
    "        if self.__frozen_blocks_num < self.__blocks_num:\n",
    "            index = self.__frozen_blocks_num.numpy()\n",
    "            block = self.blocks[index]\n",
    "            block.trainable = False\n",
    "            self.__frozen_blocks_num.assign_add(1)\n",
    "            print(\"freeze block:\", self.blocks[index].name, \", total frozen blocks:\", index+1)\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def addNewBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"add new block\")\n",
    "        i = self.__blocks_num.numpy()\n",
    "        newBlock = ResBlock(name=\"res_block\"+str(i))\n",
    "        newBlock(self.blocks[-1].output)\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num.assign_add(1)\n",
    "        print(f\"this is the {i+1} added blocks, block name: {self.blocks[i].name}\")\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def copyLastBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"copy last block\")\n",
    "        newBlock = ResBlock(name=\"res_block\"+str(self.__blocks_num.numpy()))\n",
    "        last_block:ResBlock = self.blocks[-1]\n",
    "        newBlock(last_block.output)\n",
    "        if last_block.input_shape == last_block.output_shape:\n",
    "            newBlock.set_weights(last_block.get_weights())\n",
    "        else:\n",
    "            print(\"copy failed: shape different with last block\")\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedResNet(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__blocks_num = tf.Variable(1,dtype=tf.int8, trainable=False)\n",
    "        self.__frozen_blocks_num = tf.Variable(0,dtype=tf.int8, trainable=False)\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks = [ResBlock(name=\"res_block0\")]\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(10)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # resolve output shape in model summary\n",
    "        input_layer = layers.Input(shape=input_shape[1:], batch_size=input_shape[0])\n",
    "        self.call(input_layer)\n",
    "        return super().build(input_shape)\n",
    "\n",
    "    def call(self, x=None, training=None, mask=None):\n",
    "        # 只用最后一个残差块进行训练\n",
    "        x = self.blocks[-1](x, training=training)\n",
    "        x = self.flatten(x, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        return x\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def getBlocksNum(self) -> tf.int8:\n",
    "        return self.__blocks_num\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def getLastFrozenBlock(self) -> ResBlock:\n",
    "        index = self.__frozen_blocks_num.numpy()-1\n",
    "        block = self.blocks[index]\n",
    "        return block\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def freezeBlock(self):\n",
    "        if self.__frozen_blocks_num < self.__blocks_num:\n",
    "            index = self.__frozen_blocks_num.numpy()\n",
    "            block = self.blocks[index]\n",
    "            block.trainable = False\n",
    "            self.__frozen_blocks_num.assign_add(1)\n",
    "            print(\"freeze block:\", self.blocks[index].name, \", total frozen blocks:\", index+1)\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def addNewBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"add new block\")\n",
    "        i = self.__blocks_num.numpy()\n",
    "        newBlock = ResBlock(name=\"res_block\"+str(i))\n",
    "        newBlock(self.blocks[-1].output)\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num.assign_add(1)\n",
    "        print(f\"this is the {i+1} added blocks, block name: {self.blocks[i].name}\")\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def copyLastBlock(self):\n",
    "        print(\"----------\")\n",
    "        print(\"copy last block\")\n",
    "        newBlock = ResBlock(name=\"res_block\"+str(self.__blocks_num.numpy()))\n",
    "        last_block:ResBlock = self.blocks[-1]\n",
    "        newBlock(last_block.output)\n",
    "        if last_block.input_shape == last_block.output_shape:\n",
    "            newBlock.set_weights(last_block.get_weights())\n",
    "        else:\n",
    "            print(\"copy failed: shape different with last block\")\n",
    "        self.blocks.append(newBlock)\n",
    "        self.__blocks_num.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamicResNet:\n",
    "    def __init__(self, condition: types.FunctionType = None, max_blocks_num:int = 2, cache:bool=True, copy_last_block:bool = False,*args, **wargs) -> None:\n",
    "        \"\"\"\n",
    "            condition: A function, which will be called in every epoch and returns a boolean value representing whether to add a new block.\n",
    "                        一个函数，每个epoch会被调用一次，返回值为布尔类型，代表是否添加新的块\n",
    "        \"\"\"\n",
    "        super(dynamicResNet, self).__init__(*args, **wargs)\n",
    "        if condition is None:\n",
    "            self.add_condition = self.set_epochs\n",
    "            self.add_condition()\n",
    "        else:\n",
    "            if callable(condition):\n",
    "                self.add_condition = condition\n",
    "            else:\n",
    "                raise ValueError(\"'condition' must be a function\")\n",
    "        self.max_blocks_num = max_blocks_num\n",
    "        self.use_cache = cache\n",
    "        self.copy_last_block = copy_last_block\n",
    "        # build model //创建模型\n",
    "        self.model = CachedResNet() if cache else MyResNet()\n",
    "        self.compiled = False\n",
    "\n",
    "    def compile(self,\n",
    "                optimizer=\"rmsprop\",\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                loss_weights=None,\n",
    "                weighted_metrics=None,\n",
    "                run_eagerly=None,\n",
    "                steps_per_execution=None,\n",
    "                **kwargs\n",
    "    ):\n",
    "        self.complieArgs = [optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution]\n",
    "        self.complieKwargs = kwargs\n",
    "        self.model.compile(*self.complieArgs, **kwargs)\n",
    "        self.compiled = True\n",
    "\n",
    "    def fit(self,\n",
    "            x=None,\n",
    "            y=None,\n",
    "            batch_size=None,\n",
    "            epochs=1,\n",
    "            verbose=\"auto\",\n",
    "            callbacks=None,\n",
    "            validation_split=0.0,\n",
    "            validation_data=None,\n",
    "            shuffle=True,\n",
    "            class_weight=None,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=0,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_batch_size=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False\n",
    "    ):\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError(\"model should be compiled before fit\")\n",
    "        self.epochs = epochs\n",
    "        self.fitArgs = [x,y,batch_size,1,verbose,callbacks,validation_split,validation_data,shuffle,class_weight,sample_weight,initial_epoch,steps_per_epoch,validation_steps,validation_batch_size,validation_freq,max_queue_size,workers,use_multiprocessing]\n",
    "        return self.call(training=True)\n",
    "    \n",
    "    def predict(self,\n",
    "                x,\n",
    "                batch_size=None,\n",
    "                verbose=\"auto\",\n",
    "                steps=None,\n",
    "                callbacks=None,\n",
    "                max_queue_size=10,\n",
    "                workers=1,\n",
    "                use_multiprocessing=False\n",
    "    ):\n",
    "        if not self.compiled:\n",
    "            raise RuntimeError(\"model should be compiled before predict\")\n",
    "        return self.model.predict( x,\n",
    "                                    batch_size=batch_size,\n",
    "                                    verbose=verbose,\n",
    "                                    steps=steps,\n",
    "                                    callbacks=callbacks,\n",
    "                                    max_queue_size=max_queue_size,\n",
    "                                    workers=workers,\n",
    "                                    use_multiprocessing=use_multiprocessing\n",
    "                                 )\n",
    "\n",
    "    def call(self, x=None, training=False):\n",
    "        if training:\n",
    "            if x:\n",
    "                raise ValueError(\"Please use 'fit' when training.\")\n",
    "            def fit_epoch():\n",
    "                # 满足条件动态添加新残差块\n",
    "                if self.model.getBlocksNum() < self.max_blocks_num and self.add_condition():\n",
    "                    self.model.freezeBlock()\n",
    "                    if self.copy_last_block:\n",
    "                        self.model.copyLastBlock()\n",
    "                    else:\n",
    "                        self.model.addNewBlock()\n",
    "                    if self.use_cache:\n",
    "                        print(\"caching\")\n",
    "                        block = self.model.getLastFrozenBlock()\n",
    "                        cache_model = keras.Model(block.input, block.output)\n",
    "                        self.fitArgs[0] = cache_model.predict(self.fitArgs[0], batch_size=self.fitArgs[2])\n",
    "                        print(\"cached\")\n",
    "                    print(\"compiling\")\n",
    "                    self.model.compile(*self.complieArgs, **self.complieKwargs)\n",
    "                    print(\"compiled\")\n",
    "                tf.print(self.fitArgs[0].shape)\n",
    "                self.model.fit(*self.fitArgs)\n",
    "            for epoch in range(self.epochs):\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "                # 使用多线程的方式可以释放显存\n",
    "                p = Thread(target=fit_epoch)\n",
    "                p.start()\n",
    "                p.join()\n",
    "#                 fit_epoch()\n",
    "        else:\n",
    "            return self.model.predict(x)\n",
    "\n",
    "    def set_epochs(self, interval_of_epochs:int = None) -> None:\n",
    "        self.epoch = 0\n",
    "        self.last_change_epoch = 1\n",
    "        if interval_of_epochs is None:\n",
    "            self.interval = 1\n",
    "        else:\n",
    "            self.interval = interval_of_epochs\n",
    "        self.add_condition = self.__num_of_epochs\n",
    "\n",
    "    def __num_of_epochs(self) -> bool:\n",
    "        self.epoch += 1\n",
    "        if self.epoch - self.last_change_epoch == self.interval:\n",
    "            self.last_change_epoch = self.epoch\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MyResNet.call of <__main__.MyResNet object at 0x00000253E0C1B438>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MyResNet.call of <__main__.MyResNet object at 0x00000253E0C1B438>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1563/1563 [==============================] - 27s 5ms/step - loss: 4.6037 - accuracy: 0.3448\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3310 - accuracy: 0.5498\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1591 - accuracy: 0.6033\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0348 - accuracy: 0.6446\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9247 - accuracy: 0.6788\n",
      "Epoch 6/30\n",
      "freeze blocks: 1 , total frozen blocks: 1\n",
      "----------\n",
      "add new block\n",
      "this is the 2 added blocks, block name: res_block1\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.9717 - accuracy: 0.6165\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6754 - accuracy: 0.7672\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6649 - accuracy: 0.7707\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6202 - accuracy: 0.7861\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5448 - accuracy: 0.8118\n",
      "Epoch 11/30\n",
      "freeze blocks: 1 , total frozen blocks: 2\n",
      "----------\n",
      "add new block\n",
      "this is the 3 added blocks, block name: res_block2\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 6.9341 - accuracy: 0.6146\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9199 - accuracy: 0.8391\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4944 - accuracy: 0.8794\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3381 - accuracy: 0.9020\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2534 - accuracy: 0.9217\n",
      "Epoch 16/30\n",
      "freeze blocks: 1 , total frozen blocks: 3\n",
      "----------\n",
      "add new block\n",
      "this is the 4 added blocks, block name: res_block3\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 5.8653 - accuracy: 0.6971\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8045 - accuracy: 0.9031\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.4669 - accuracy: 0.9265\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3065 - accuracy: 0.9409\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2771 - accuracy: 0.9438\n",
      "Epoch 21/30\n",
      "freeze blocks: 1 , total frozen blocks: 4\n",
      "----------\n",
      "add new block\n",
      "this is the 5 added blocks, block name: res_block4\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 4.6342 - accuracy: 0.7807\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.7221 - accuracy: 0.9320\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4494 - accuracy: 0.9491\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3161 - accuracy: 0.9556\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2408 - accuracy: 0.9623\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2229 - accuracy: 0.9642\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1712 - accuracy: 0.9689\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1662 - accuracy: 0.9700\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1510 - accuracy: 0.9731\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1202 - accuracy: 0.9768\n",
      "Model: \"my_res_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "res_block0 (ResBlock)        (None, 32, 32, 64)        2560      \n",
      "_________________________________________________________________\n",
      "res_block1 (ResBlock)        (None, 32, 32, 64)        37184     \n",
      "_________________________________________________________________\n",
      "res_block2 (ResBlock)        (None, 32, 32, 64)        37184     \n",
      "_________________________________________________________________\n",
      "res_block3 (ResBlock)        (None, 32, 32, 64)        37184     \n",
      "_________________________________________________________________\n",
      "res_block4 (ResBlock)        (None, 32, 32, 64)        37184     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                655370    \n",
      "=================================================================\n",
      "Total params: 806,666\n",
      "Trainable params: 692,426\n",
      "Non-trainable params: 114,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dynamic_model = dynamicResNet(max_blocks_num=5, cache=False, copy_last_block=False)\n",
    "dynamic_model.set_epochs(5)\n",
    "def fit_dinamic_model():\n",
    "    dynamic_model.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    dynamic_model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "p = Thread(target=fit_dinamic_model)\n",
    "p.start()\n",
    "p.join()\n",
    "dynamic_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "(5000, 32, 32, 3, 1)\n",
      "157/157 [==============================] - 17s 12ms/step - loss: 31.1871 - accuracy: 0.2472\n",
      "Epoch 2/3\n",
      "freeze block: res_block0 , total frozen blocks: 1\n",
      "----------\n",
      "add new block\n",
      "this is the 2 added blocks, block name: res_block1\n",
      "caching\n",
      "cached\n",
      "compiling\n",
      "compiled\n",
      "(5000, 32, 32, 3, 64)\n",
      "157/157 [==============================] - 5s 21ms/step - loss: 4.4913 - accuracy: 0.3568\n",
      "Epoch 3/3\n",
      "freeze block: res_block1 , total frozen blocks: 2\n",
      "----------\n",
      "add new block\n",
      "this is the 3 added blocks, block name: res_block2\n",
      "caching\n",
      "WARNING:tensorflow:Functional model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model\" was not an Input tensor, it was generated by layer res_block0.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: res_block0/add:0\n",
      "Model: \"cached_res_net_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "res_block0 (ResBlock)        (None, 32, 32, 3, 64)     1280      \n",
      "_________________________________________________________________\n",
      "res_block1 (ResBlock)        (None, 32, 32, 3, 64)     37184     \n",
      "_________________________________________________________________\n",
      "res_block2 (ResBlock)        (None, 32, 32, 3, 64)     37184     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 196608)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1966090   \n",
      "=================================================================\n",
      "Total params: 2,041,740\n",
      "Trainable params: 2,003,146\n",
      "Non-trainable params: 38,594\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-22-bf8b7a2b7f0a>\", line 102, in fit_epoch\n",
      "    cache_model = keras.Model(block.input, block.output)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 522, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 115, in __init__\n",
      "    self._init_graph_network(inputs, outputs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 522, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 199, in _init_graph_network\n",
      "    self.inputs, self.outputs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py\", line 990, in _map_graph_network\n",
      "    str(layers_with_complete_input))\n",
      "ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 3, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"res_block0\". The following previous layers were accessed without issue: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic_model_cache = dynamicResNet(max_blocks_num=5, cache=True, copy_last_block=False)\n",
    "dynamic_model_cache.set_epochs(1)\n",
    "def fit_dinamic_model_cache():\n",
    "    dynamic_model_cache.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    dynamic_model_cache.fit(train_x, train_y, batch_size=batch_size, epochs=3, callbacks=[tensorboard_callback])\n",
    "p = Thread(target=fit_dinamic_model_cache)\n",
    "p.start()\n",
    "p.join()\n",
    "dynamic_model_cache.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dynamic_model_copy = dynamicResNet(max_blocks_num=5, copy_last_block=True)\n",
    "dynamic_model_copy.set_epochs(5)\n",
    "\n",
    "def fit_dinamic_model_copy():\n",
    "    dynamic_model_copy.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    dynamic_model_copy.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "\n",
    "p = Thread(target=fit_dinamic_model_copy)\n",
    "p.start()\n",
    "p.join()\n",
    "dynamic_model_copy.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_model_cache_copy = dynamicResNet(max_blocks_num=5, copy_last_block=False)\n",
    "dynamic_model_cache_copy.set_epochs(5)\n",
    "def fit_dynamic_model_cache_copy():\n",
    "    dynamic_model_cache_copy.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    dynamic_model_cache_copy.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "p = Thread(target=fit_dynamic_model_cache_copy)\n",
    "p.start()\n",
    "p.join()\n",
    "dynamic_model_cache.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_model = Sequential([ResBlock(), ResBlock(), ResBlock(), ResBlock(), ResBlock(), layers.Flatten(), layers.Dense(10)])\n",
    "def fit_static_model():\n",
    "    static_model.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    static_model.fit(train_x, train_y, batch_size=batch_size, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "p = Thread(target=fit_static_model)\n",
    "p.start()\n",
    "p.join()\n",
    "static_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "157/157 [==============================] - 7s 15ms/step - loss: 28.4924 - accuracy: 0.2222\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 2.3798 - accuracy: 0.4617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23517da3e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = MyResNet(use_cache=False)\n",
    "test_model.compile(optimizer=\"Adam\", loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "test_model.fit(train_x, train_y, batch_size=batch_size, epochs=2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = Model(test_model.layers[0].input, test_model.layers[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ty=test2.predict(test_x,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(dynamic_model_cache.model.getBlocksNum().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "887b14c3bf155c2c86854178e2c4969ec2771ba32ccfe4ffdf11cae456a2e83f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
