{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchinfo\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device:【cuda:None】\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU Device:【{}:{}】\".format(device.type, device.index))\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data:np.ndarray, labels:np.ndarray, transform=ToTensor(), \n",
    "    target_transform=Lambda(lambda y: torch.zeros(2, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))):\n",
    "        self.data:torch.Tensor = torch.from_numpy(data)\n",
    "        self.labels:torch.Tensor = torch.from_numpy(labels)\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "        # self.transform = transform\n",
    "        # self.target_transform = target_transform\n",
    "        # self.shuffle()\n",
    "    \n",
    "    def shuffle(self, seed=None):\n",
    "        '\\n        seed(self, seed=None)\\n\\n        Reseed a legacy MT19937 BitGenerator\\n        '\n",
    "        self.shuffle_seed = np.random.randint(1, 65535) if seed is None else seed\n",
    "        print(f\"随机种子：{self.shuffle_seed}\")\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.data)\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx, 0]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=\"dataset.npz\", train_percent=0.8) -> tuple:\n",
    "    with np.load(path) as dataset:\n",
    "        full_data = dataset[\"data\"].astype(np.float32).reshape((-1, 1, 116, 116))\n",
    "        full_labels = dataset[\"labels\"].astype(np.int64)\n",
    "    train_size = int(full_data.shape[0]*train_percent)\n",
    "    test_size = full_data.shape[0]-train_size\n",
    "    seed = np.random.randint(1, 65535)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_data)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_labels)\n",
    "    train_data, test_data = full_data[:train_size], full_data[train_size:]\n",
    "    train_labels, test_labels = full_labels[:train_size], full_labels[train_size:]\n",
    "    print(f\"训练集大小：{train_size}\", f\"测试集大小：{test_size}\", f\"随机种子：{seed}\")\n",
    "    train_dataset = CustomDataset(train_data, train_labels)\n",
    "    test_dataset = CustomDataset(test_data, test_labels)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：706 测试集大小：177 随机种子：19746\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_FC_dataset.npz\", 0.8)\n",
    "# train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_FC_augmented_dataset.npz\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 116, 116])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    data_shape = X.shape\n",
    "    label_shape = y.shape\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LKA(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()        \n",
    "        attn = self.conv0(x)\n",
    "        attn = self.conv_spatial(attn)\n",
    "        attn = self.conv1(attn)\n",
    "\n",
    "        return u * attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_1 = nn.Conv2d(d_model, d_model, 1)\n",
    "        self.activation = nn.GELU()\n",
    "        self.spatial_gating_unit = LKA(d_model)\n",
    "        self.proj_2 = nn.Conv2d(d_model, d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shorcut = x.clone()\n",
    "        x = self.proj_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.spatial_gating_unit(x)\n",
    "        x = self.proj_2(x)\n",
    "        x = x + shorcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(dim)\n",
    "        self.attn = Attention(dim)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm2d(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=patch_size//2)\n",
    "        self.norm = nn.BatchNorm2d(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        x = self.norm(x)        \n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAN(nn.Module):\n",
    "    def __init__(self, img_size=224, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n",
    "                mlp_ratios=[4, 4, 4, 4], drop_rate=0., norm_layer=nn.LayerNorm,\n",
    "                 depths=[3, 4, 6, 3], num_stages=4):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        for i in range(num_stages):\n",
    "            patch_embed = OverlapPatchEmbed(img_size=img_size if i == 0 else img_size // (2 ** (i + 1)),\n",
    "                                            patch_size=7 if i == 0 else 3,\n",
    "                                            stride=4 if i == 0 else 2,\n",
    "                                            in_chans=in_chans if i == 0 else embed_dims[i - 1],\n",
    "                                            embed_dim=embed_dims[i])\n",
    "\n",
    "            block = nn.ModuleList([Block(dim=embed_dims[i], mlp_ratio=mlp_ratios[i], drop=drop_rate)\n",
    "                for j in range(depths[i])])\n",
    "            norm = norm_layer(embed_dims[i])\n",
    "\n",
    "            setattr(self, f\"patch_embed{i + 1}\", patch_embed)\n",
    "            setattr(self, f\"block{i + 1}\", block)\n",
    "            setattr(self, f\"norm{i + 1}\", norm)\n",
    "\n",
    "        # classification head\n",
    "        self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embed = getattr(self, f\"patch_embed{i + 1}\")\n",
    "            block = getattr(self, f\"block{i + 1}\")\n",
    "            norm = getattr(self, f\"norm{i + 1}\")\n",
    "            x, H, W = patch_embed(x)\n",
    "            for blk in block:\n",
    "                x = blk(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = norm(x)\n",
    "            if i != self.num_stages - 1:\n",
    "                x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return x.mean(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader:DataLoader, loss_fn, device=torch.device(\"cuda\")):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model.forward(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "12/12  [706/706] - batch loss: 0.745088 - batch accuracy: 50.0% - 195.912ms/batch\n",
      "-- Average loss: 0.763120 - Accuracy: 51.0% - 2351.938ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 0.702497 \n",
      "\n",
      "Epoch: 2/20\n",
      "12/12  [706/706] - batch loss: 0.001821 - batch accuracy: 100.0% - 178.970ms/batch\n",
      "-- Average loss: 0.384268 - Accuracy: 99.0% - 2148.643ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 0.777211 \n",
      "\n",
      "Epoch: 3/20\n",
      "12/12  [706/706] - batch loss: 0.000117 - batch accuracy: 100.0% - 177.954ms/batch\n",
      "-- Average loss: 0.138901 - Accuracy: 100.0% - 2136.451ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.4%, Avg loss: 0.838158 \n",
      "\n",
      "Epoch: 4/20\n",
      "12/12  [706/706] - batch loss: 0.000022 - batch accuracy: 100.0% - 177.797ms/batch\n",
      "-- Average loss: 0.014843 - Accuracy: 100.0% - 2134.565ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.851510 \n",
      "\n",
      "Epoch: 5/20\n",
      "12/12  [706/706] - batch loss: 0.000012 - batch accuracy: 100.0% - 178.074ms/batch\n",
      "-- Average loss: 0.002325 - Accuracy: 100.0% - 2137.891ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.892968 \n",
      "\n",
      "Epoch: 6/20\n",
      "12/12  [706/706] - batch loss: 0.000009 - batch accuracy: 100.0% - 176.079ms/batch\n",
      "-- Average loss: 0.000993 - Accuracy: 100.0% - 2113.955ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.938318 \n",
      "\n",
      "Epoch: 7/20\n",
      "12/12  [706/706] - batch loss: 0.000008 - batch accuracy: 100.0% - 177.583ms/batch\n",
      "-- Average loss: 0.000693 - Accuracy: 100.0% - 2132.006ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.1%, Avg loss: 0.957184 \n",
      "\n",
      "Epoch: 8/20\n",
      "12/12  [706/706] - batch loss: 0.000008 - batch accuracy: 100.0% - 176.514ms/batch\n",
      "-- Average loss: 0.000577 - Accuracy: 100.0% - 2119.169ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.965574 \n",
      "\n",
      "Epoch: 9/20\n",
      "12/12  [706/706] - batch loss: 0.000007 - batch accuracy: 100.0% - 179.236ms/batch\n",
      "-- Average loss: 0.000511 - Accuracy: 100.0% - 2150.836ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.971761 \n",
      "\n",
      "Epoch: 10/20\n",
      "12/12  [706/706] - batch loss: 0.000007 - batch accuracy: 100.0% - 177.823ms/batch\n",
      "-- Average loss: 0.000464 - Accuracy: 100.0% - 2134.877ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 0.978742 \n",
      "\n",
      "Epoch: 11/20\n",
      "12/12  [706/706] - batch loss: 0.000007 - batch accuracy: 100.0% - 176.492ms/batch\n",
      "-- Average loss: 0.000424 - Accuracy: 100.0% - 2118.905ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.986002 \n",
      "\n",
      "Epoch: 12/20\n",
      "12/12  [706/706] - batch loss: 0.000006 - batch accuracy: 100.0% - 177.056ms/batch\n",
      "-- Average loss: 0.000390 - Accuracy: 100.0% - 2125.671ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.993264 \n",
      "\n",
      "Epoch: 13/20\n",
      "12/12  [706/706] - batch loss: 0.000006 - batch accuracy: 100.0% - 177.337ms/batch\n",
      "-- Average loss: 0.000360 - Accuracy: 100.0% - 2128.039ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.000418 \n",
      "\n",
      "Epoch: 14/20\n",
      "12/12  [706/706] - batch loss: 0.000006 - batch accuracy: 100.0% - 176.774ms/batch\n",
      "-- Average loss: 0.000334 - Accuracy: 100.0% - 2122.285ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.007445 \n",
      "\n",
      "Epoch: 15/20\n",
      "12/12  [706/706] - batch loss: 0.000006 - batch accuracy: 100.0% - 177.249ms/batch\n",
      "-- Average loss: 0.000310 - Accuracy: 100.0% - 2127.997ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.014314 \n",
      "\n",
      "Epoch: 16/20\n",
      "12/12  [706/706] - batch loss: 0.000005 - batch accuracy: 100.0% - 176.483ms/batch\n",
      "-- Average loss: 0.000288 - Accuracy: 100.0% - 2118.802ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.021017 \n",
      "\n",
      "Epoch: 17/20\n",
      "12/12  [706/706] - batch loss: 0.000005 - batch accuracy: 100.0% - 176.641ms/batch\n",
      "-- Average loss: 0.000269 - Accuracy: 100.0% - 2119.693ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.027557 \n",
      "\n",
      "Epoch: 18/20\n",
      "12/12  [706/706] - batch loss: 0.000005 - batch accuracy: 100.0% - 176.914ms/batch\n",
      "-- Average loss: 0.000252 - Accuracy: 100.0% - 2123.962ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.033963 \n",
      "\n",
      "Epoch: 19/20\n",
      "12/12  [706/706] - batch loss: 0.000005 - batch accuracy: 100.0% - 177.354ms/batch\n",
      "-- Average loss: 0.000236 - Accuracy: 100.0% - 2128.246ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.040222 \n",
      "\n",
      "Epoch: 20/20\n",
      "12/12  [706/706] - batch loss: 0.000005 - batch accuracy: 100.0% - 177.489ms/batch\n",
      "-- Average loss: 0.000222 - Accuracy: 100.0% - 2130.873ms\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.046331 \n",
      "\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "VAN                                      --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "├─ModuleList: 1-4                        --                        --\n",
      "├─OverlapPatchEmbed: 1-5                 [64, 64, 29, 29]          --\n",
      "│    └─Conv2d: 2-1                       [64, 64, 29, 29]          3,200\n",
      "│    └─BatchNorm2d: 2-2                  [64, 64, 29, 29]          128\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─Block: 2-3                        [64, 64, 29, 29]          --\n",
      "│    │    └─BatchNorm2d: 3-1             [64, 64, 29, 29]          128\n",
      "│    │    └─Attention: 3-2               [64, 64, 29, 29]          17,344\n",
      "│    │    └─BatchNorm2d: 3-3             [64, 64, 29, 29]          128\n",
      "│    │    └─Mlp: 3-4                     [64, 64, 29, 29]          35,648\n",
      "│    └─Block: 2-4                        [64, 64, 29, 29]          --\n",
      "│    │    └─BatchNorm2d: 3-5             [64, 64, 29, 29]          128\n",
      "│    │    └─Attention: 3-6               [64, 64, 29, 29]          17,344\n",
      "│    │    └─BatchNorm2d: 3-7             [64, 64, 29, 29]          128\n",
      "│    │    └─Mlp: 3-8                     [64, 64, 29, 29]          35,648\n",
      "│    └─Block: 2-5                        [64, 64, 29, 29]          --\n",
      "│    │    └─BatchNorm2d: 3-9             [64, 64, 29, 29]          128\n",
      "│    │    └─Attention: 3-10              [64, 64, 29, 29]          17,344\n",
      "│    │    └─BatchNorm2d: 3-11            [64, 64, 29, 29]          128\n",
      "│    │    └─Mlp: 3-12                    [64, 64, 29, 29]          35,648\n",
      "├─LayerNorm: 1-6                         [64, 841, 64]             128\n",
      "├─OverlapPatchEmbed: 1-7                 [64, 128, 15, 15]         --\n",
      "│    └─Conv2d: 2-6                       [64, 128, 15, 15]         73,856\n",
      "│    └─BatchNorm2d: 2-7                  [64, 128, 15, 15]         256\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "│    └─Block: 2-8                        [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-13            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-14              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-15            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-16                    [64, 128, 15, 15]         136,832\n",
      "│    └─Block: 2-9                        [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-17            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-18              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-19            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-20                    [64, 128, 15, 15]         136,832\n",
      "│    └─Block: 2-10                       [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-21            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-22              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-23            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-24                    [64, 128, 15, 15]         136,832\n",
      "│    └─Block: 2-11                       [64, 128, 15, 15]         --\n",
      "│    │    └─BatchNorm2d: 3-25            [64, 128, 15, 15]         256\n",
      "│    │    └─Attention: 3-26              [64, 128, 15, 15]         59,264\n",
      "│    │    └─BatchNorm2d: 3-27            [64, 128, 15, 15]         256\n",
      "│    │    └─Mlp: 3-28                    [64, 128, 15, 15]         136,832\n",
      "├─LayerNorm: 1-8                         [64, 225, 128]            256\n",
      "├─OverlapPatchEmbed: 1-9                 [64, 256, 8, 8]           --\n",
      "│    └─Conv2d: 2-12                      [64, 256, 8, 8]           295,168\n",
      "│    └─BatchNorm2d: 2-13                 [64, 256, 8, 8]           512\n",
      "├─ModuleList: 1-3                        --                        --\n",
      "│    └─Block: 2-14                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-29            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-30              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-31            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-32                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-15                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-33            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-34              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-35            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-36                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-16                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-37            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-38              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-39            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-40                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-17                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-41            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-42              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-43            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-44                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-18                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-45            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-46              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-47            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-48                    [64, 256, 8, 8]           535,808\n",
      "│    └─Block: 2-19                       [64, 256, 8, 8]           --\n",
      "│    │    └─BatchNorm2d: 3-49            [64, 256, 8, 8]           512\n",
      "│    │    └─Attention: 3-50              [64, 256, 8, 8]           216,832\n",
      "│    │    └─BatchNorm2d: 3-51            [64, 256, 8, 8]           512\n",
      "│    │    └─Mlp: 3-52                    [64, 256, 8, 8]           535,808\n",
      "├─LayerNorm: 1-10                        [64, 64, 256]             512\n",
      "├─OverlapPatchEmbed: 1-11                [64, 512, 4, 4]           --\n",
      "│    └─Conv2d: 2-20                      [64, 512, 4, 4]           1,180,160\n",
      "│    └─BatchNorm2d: 2-21                 [64, 512, 4, 4]           1,024\n",
      "├─ModuleList: 1-4                        --                        --\n",
      "│    └─Block: 2-22                       [64, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-53            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Attention: 3-54              [64, 512, 4, 4]           826,880\n",
      "│    │    └─BatchNorm2d: 3-55            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Mlp: 3-56                    [64, 512, 4, 4]           2,120,192\n",
      "│    └─Block: 2-23                       [64, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-57            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Attention: 3-58              [64, 512, 4, 4]           826,880\n",
      "│    │    └─BatchNorm2d: 3-59            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Mlp: 3-60                    [64, 512, 4, 4]           2,120,192\n",
      "│    └─Block: 2-24                       [64, 512, 4, 4]           --\n",
      "│    │    └─BatchNorm2d: 3-61            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Attention: 3-62              [64, 512, 4, 4]           826,880\n",
      "│    │    └─BatchNorm2d: 3-63            [64, 512, 4, 4]           1,024\n",
      "│    │    └─Mlp: 3-64                    [64, 512, 4, 4]           2,120,192\n",
      "├─LayerNorm: 1-12                        [64, 16, 512]             1,024\n",
      "├─Linear: 1-13                           [64, 2]                   1,026\n",
      "==========================================================================================\n",
      "Total params: 15,872,770\n",
      "Trainable params: 15,872,770\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 51.06\n",
      "==========================================================================================\n",
      "Input size (MB): 3.44\n",
      "Forward/backward pass size (MB): 3437.79\n",
      "Params size (MB): 63.49\n",
      "Estimated Total Size (MB): 3504.73\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = VAN(data_shape[2], data_shape[1], 2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "model.to(device)\n",
    "\n",
    "size = len(train_dataloader.dataset)\n",
    "num_batches = size // batch_size\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "    loss, correct = 0, 0\n",
    "    time_start = time.time()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # time_batch_start = time.time()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model.forward(X)\n",
    "        batch_loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        current = batch * batch_size + len(X)\n",
    "\n",
    "        batch_loss = batch_loss.item()\n",
    "        loss += batch_loss\n",
    "        batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        correct += batch_correct\n",
    "\n",
    "        batch_correct /= len(X)\n",
    "        # time_batch_end = time.time()\n",
    "        time_end = time.time()\n",
    "        print(f\"\\r{batch+1}/{num_batches+1}  [{current:>3d}/{size:>3d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}% - {(time_end-time_start)/(batch+1)*1000:>0.3f}ms/batch\", end = \"\", flush=True)\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    time_end = time.time()\n",
    "    print(f\"\\n-- Average loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}% - {(time_end-time_start)*1000:>0.3f}ms\\n\")\n",
    "    test(model, test_dataloader, loss_fn, device=device)\n",
    "print(torchinfo.summary(model, data_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000368 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Avg loss: 1.046331 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, train_dataloader, loss_fn)\n",
    "test(model, test_dataloader, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
