{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import torchinfo\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device:【cuda:None】\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU Device:【{}:{}】\".format(device.type, device.index))\n",
    "    torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data:np.ndarray, labels:np.ndarray, transform=ToTensor(), \n",
    "    target_transform=Lambda(lambda y: torch.zeros(2, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))):\n",
    "        self.data:torch.Tensor = torch.from_numpy(data)\n",
    "        self.labels:torch.Tensor = torch.from_numpy(labels)\n",
    "        self.transform = None\n",
    "        self.target_transform = None\n",
    "        # self.transform = transform\n",
    "        # self.target_transform = target_transform\n",
    "        # self.shuffle()\n",
    "    \n",
    "    def shuffle(self, seed=None):\n",
    "        '\\n        seed(self, seed=None)\\n\\n        Reseed a legacy MT19937 BitGenerator\\n        '\n",
    "        self.shuffle_seed = np.random.randint(1, 65535) if seed is None else seed\n",
    "        print(f\"随机种子：{self.shuffle_seed}\")\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.data)\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        np.random.shuffle(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx, 0]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path=\"dataset.npz\", train_percent=0.8) -> tuple:\n",
    "    with np.load(path) as dataset:\n",
    "        full_data = dataset[\"data\"].astype(np.float32).reshape((-1, 1, 116, 116))\n",
    "        full_labels = dataset[\"labels\"].astype(np.int64)\n",
    "    train_size = int(full_data.shape[0]*train_percent)\n",
    "    test_size = full_data.shape[0]-train_size\n",
    "    seed = np.random.randint(1, 65535)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_data)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(full_labels)\n",
    "    train_data, test_data = full_data[:train_size], full_data[train_size:]\n",
    "    train_labels, test_labels = full_labels[:train_size], full_labels[train_size:]\n",
    "    print(f\"训练集大小：{train_size}\", f\"测试集大小：{test_size}\", f\"随机种子：{seed}\")\n",
    "    train_dataset = CustomDataset(train_data, train_labels)\n",
    "    test_dataset = CustomDataset(test_data, test_labels)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小：9636 测试集大小：2409 随机种子：11000\n"
     ]
    }
   ],
   "source": [
    "# train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_FC_dataset.npz\", 0.8)\n",
    "train_dataset, test_dataset = load_dataset(\"D:\\\\datasets\\\\ABIDE\\\\ABIDE_FC_augmented_dataset.npz\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 116, 116])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    data_shape = X.shape\n",
    "    label_shape = y.shape\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim:int):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LKA(nn.Module):\n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)\n",
    "        self.conv_spatial = nn.Conv2d(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.clone()\n",
    "        attn = self.conv0(x)\n",
    "        attn = self.conv_spatial(attn)\n",
    "        attn = self.conv1(attn)\n",
    "\n",
    "        return u * attn, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, out_channels:int=None, device=None, *args, **wargs):\n",
    "        super().__init__(*args, **wargs)\n",
    "        self.__built = False\n",
    "        self.out_channels = out_channels\n",
    "        self.device = device\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        in_channels = input_shape[1]\n",
    "        out_channels = in_channels if self.out_channels is None else in_channels\n",
    "        # resolve output shape in model summary\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.lka = LKA(in_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.downconv = DWConv(in_channels)\n",
    "        self.output_shape = input_shape\n",
    "        self.__built = True\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x:np.ndarray):\n",
    "        fx:np.ndarray = x\n",
    "        fx = self.conv(fx)\n",
    "        fx, attn = self.lka(fx)\n",
    "        fx = self.bn(fx)\n",
    "        if fx.shape[-1] != x.shape[-1]:\n",
    "            x = self.downconv(x)\n",
    "        return fx + x, attn\n",
    "\n",
    "    def freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LKAResNet(nn.Module):\n",
    "    def __init__(self, device=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__built:bool = False\n",
    "        self.device:torch.DeviceObjType = device\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # self.lstm = nn.LSTM(116, 116, 2, batch_first=True)\n",
    "        # An ordinary ResNet, but put blocks in a list. New blocks will be added into this list when training.\n",
    "        # 常规的残差网络，但将残差块放在一个list中，训练时会将新块添加到这里\n",
    "        self.blocks:nn.ModuleList = nn.ModuleList([ResBlock() for _ in range(2)])\n",
    "        for block in self.blocks:\n",
    "            block.build(input_shape)\n",
    "            block.to(self.device)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(np.prod(input_shape[1:]), 2)\n",
    "        self.__built = True\n",
    "\n",
    "    def compile(self, dataloader:DataLoader, loss_fn, optimizer, lr=1e-2):\n",
    "        self.batch_size:int = dataloader.batch_size\n",
    "        for X, y in dataloader:\n",
    "            self.input_shape:tuple = X.shape\n",
    "            self.output_shape:tuple = y.shape\n",
    "            break\n",
    "        self.build(self.input_shape)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x, (h_n, c_n) = self.lstm(x)\n",
    "        for blk in self.blocks:\n",
    "            x, attn = blk(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def get_attn(self, x):\n",
    "        self.eval()\n",
    "        attention = []\n",
    "        with torch.no_grad():\n",
    "            for blk in self.blocks:\n",
    "                x, attn = blk(x)\n",
    "                attention.append(attn.cpu().numpy().reshape((-1, 116, 116)))\n",
    "        return np.array(attention)\n",
    "\n",
    "    def fit(self, dataloader:DataLoader, epochs:int=1, test_dataloader=None):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = size // self.batch_size\n",
    "        time_collection = []\n",
    "        loss_collection = []\n",
    "        correct_collection = []\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}\")\n",
    "            self.train()\n",
    "            loss, correct = 0, 0\n",
    "            time_delta = 0\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                X = X.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # 计时\n",
    "                torch.cuda.synchronize()\n",
    "                time_start = time.time()\n",
    "\n",
    "                # Compute prediction error\n",
    "                pred = self.forward(X)\n",
    "                batch_loss = self.loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计时结束\n",
    "                torch.cuda.synchronize()\n",
    "                time_end = time.time()\n",
    "\n",
    "                current = batch * self.batch_size + len(X)\n",
    "\n",
    "                batch_loss = batch_loss.item()\n",
    "                loss += batch_loss\n",
    "\n",
    "                batch_correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                correct += batch_correct\n",
    "                batch_correct /= len(X)\n",
    "\n",
    "                batch_time = time_end - time_start\n",
    "                time_delta += batch_time\n",
    "                print(f\"\\r{batch+1}/{num_batches+1}  [{current:>3d}/{size:>3d}] - batch loss: {batch_loss:>7f} - batch accuracy: {(100*batch_correct):>0.1f}% - {batch_time*1000:>0.3f}ms\", end = \"\", flush=True)\n",
    "            loss /= num_batches\n",
    "            correct /= size\n",
    "            print(f\"\\n-- Average loss: {loss:>7f} - Accuracy: {(100*correct):>0.1f}% - {time_delta/num_batches*1000:>0.3f}ms/batch\")\n",
    "            time_collection.append(time_delta)\n",
    "            loss_collection.append(loss)\n",
    "            correct_collection.append(correct)\n",
    "            if test_dataloader is not None:\n",
    "                self.test(test_dataloader)\n",
    "        print(\"\\n\", torchinfo.summary(self, input_size=self.input_shape))\n",
    "        return correct_collection, loss_collection, time_collection\n",
    "\n",
    "    def test(self, dataloader:DataLoader, return_preds=False):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        ys = []\n",
    "        preds = []\n",
    "        self.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                if return_preds:\n",
    "                    ys = np.hstack((ys, y.cpu()))\n",
    "                    preds = np.hstack((preds, pred.argmax(1).cpu()))\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        print(f\"Test Accuracy: {(100*correct):>0.1f}%, Average loss: {test_loss:>8f} \\n\")\n",
    "        if return_preds:\n",
    "            return ys, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, loss, timing = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "151/151  [9636/9636] - batch loss: 0.544637 - batch accuracy: 80.6% - 23.000ms\n",
      "-- Average loss: 0.594560 - Accuracy: 70.8% - 45.430ms/batch\n",
      "Test Accuracy: 77.6%, Average loss: 0.513704 \n",
      "\n",
      "Epoch: 2/5\n",
      "151/151  [9636/9636] - batch loss: 0.419087 - batch accuracy: 91.7% - 23.998ms\n",
      "-- Average loss: 0.452903 - Accuracy: 83.2% - 44.006ms/batch\n",
      "Test Accuracy: 83.1%, Average loss: 0.420595 \n",
      "\n",
      "Epoch: 3/5\n",
      "151/151  [9636/9636] - batch loss: 0.324374 - batch accuracy: 94.4% - 23.000ms\n",
      "-- Average loss: 0.362488 - Accuracy: 88.0% - 44.250ms/batch\n",
      "Test Accuracy: 86.8%, Average loss: 0.352300 \n",
      "\n",
      "Epoch: 4/5\n",
      "151/151  [9636/9636] - batch loss: 0.251975 - batch accuracy: 97.2% - 25.000ms\n",
      "-- Average loss: 0.295222 - Accuracy: 91.1% - 45.255ms/batch\n",
      "Test Accuracy: 89.7%, Average loss: 0.298740 \n",
      "\n",
      "Epoch: 5/5\n",
      "151/151  [9636/9636] - batch loss: 0.196415 - batch accuracy: 97.2% - 24.001ms\n",
      "-- Average loss: 0.242222 - Accuracy: 93.4% - 45.017ms/batch\n",
      "Test Accuracy: 91.8%, Average loss: 0.254690 \n",
      "\n",
      "\n",
      " ==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LKAResNet                                --                        --\n",
      "├─ModuleList: 1-1                        --                        --\n",
      "│    └─ResBlock: 2-1                     [64, 1, 116, 116]         --\n",
      "│    │    └─Conv2d: 3-1                  [64, 1, 116, 116]         2\n",
      "│    │    └─LKA: 3-2                     [64, 1, 116, 116]         78\n",
      "│    │    └─BatchNorm2d: 3-3             [64, 1, 116, 116]         2\n",
      "│    └─ResBlock: 2-2                     [64, 1, 116, 116]         --\n",
      "│    │    └─Conv2d: 3-4                  [64, 1, 116, 116]         2\n",
      "│    │    └─LKA: 3-5                     [64, 1, 116, 116]         78\n",
      "│    │    └─BatchNorm2d: 3-6             [64, 1, 116, 116]         2\n",
      "├─Flatten: 1-2                           [64, 13456]               --\n",
      "├─Linear: 1-3                            [64, 2]                   26,914\n",
      "==========================================================================================\n",
      "Total params: 27,078\n",
      "Trainable params: 27,078\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 139.51\n",
      "==========================================================================================\n",
      "Input size (MB): 3.44\n",
      "Forward/backward pass size (MB): 68.90\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 72.45\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "base_model = LKAResNet(device)\n",
    "base_model.compile(train_dataloader, loss_fn=loss_fn, optimizer=optimizer, lr=lr)\n",
    "correct[\"RLKA\"], loss[\"RLKA\"], timing[\"RLKA\"] = base_model.fit(train_dataloader, epochs=epochs, test_dataloader=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.8%, Average loss: 0.210122 \n",
      "\n",
      "Test Accuracy: 91.8%, Average loss: 0.254690 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model.test(train_dataloader)\n",
    "base_model.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in test_dataloader:\n",
    "    attn = np.array(base_model.get_attn(x.to(device)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./attention.pkl\", \"wb\") as f:\n",
    "    pickle.dump(attn, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
